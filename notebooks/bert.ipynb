{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Pre-Training from scratch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for GPU and libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "========================================= ROCm System Management Interface =========================================\n",
      "=================================================== Concise Info ===================================================\n",
      "Device  Node  IDs              Temp    Power  Partitions          SCLK     MCLK     Fan  Perf  PwrCap  VRAM%  GPU%  \n",
      "\u001b[3m              (DID,     GUID)  (Edge)  (Avg)  (Mem, Compute, ID)                                                    \u001b[0m\n",
      "====================================================================================================================\n",
      "0       4     0x740c,   57586  37.0°C  95.0W  N/A, N/A, 0         800Mhz   1600Mhz  0%   auto  560.0W  5%     0%    \n",
      "1       5     0x740c,   45873  39.0°C  N/A    N/A, N/A, 0         1700Mhz  1600Mhz  0%   auto  0.0W    5%     9%    \n",
      "2       2     0x740c,   14571  40.0°C  92.0W  N/A, N/A, 0         800Mhz   1600Mhz  0%   auto  560.0W  7%     0%    \n",
      "3       3     0x740c,   27432  35.0°C  N/A    N/A, N/A, 0         800Mhz   1600Mhz  0%   auto  0.0W    6%     0%    \n",
      "4       8     0x740c,   30939  33.0°C  86.0W  N/A, N/A, 0         800Mhz   1600Mhz  0%   auto  560.0W  5%     0%    \n",
      "5       9     0x740c,   8466   33.0°C  N/A    N/A, N/A, 0         800Mhz   1600Mhz  0%   auto  0.0W    6%     0%    \n",
      "6       6     0x740c,   41154  34.0°C  97.0W  N/A, N/A, 0         800Mhz   1600Mhz  0%   auto  560.0W  6%     0%    \n",
      "7       7     0x740c,   63755  41.0°C  N/A    N/A, N/A, 0         800Mhz   1600Mhz  0%   auto  0.0W    5%     0%    \n",
      "====================================================================================================================\n",
      "=============================================== End of ROCm SMI Log ================================================\n"
     ]
    }
   ],
   "source": [
    "!rocm-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:            x86_64\n",
      "  CPU op-mode(s):        32-bit, 64-bit\n",
      "  Address sizes:         48 bits physical, 48 bits virtual\n",
      "  Byte Order:            Little Endian\n",
      "CPU(s):                  128\n",
      "  On-line CPU(s) list:   0-127\n",
      "Vendor ID:               AuthenticAMD\n",
      "  Model name:            AMD EPYC 7763 64-Core Processor\n",
      "    CPU family:          25\n",
      "    Model:               1\n",
      "    Thread(s) per core:  1\n",
      "    Core(s) per socket:  64\n",
      "    Socket(s):           2\n",
      "    Stepping:            1\n",
      "    Frequency boost:     enabled\n",
      "    CPU max MHz:         2450.0000\n",
      "    CPU min MHz:         1500.0000\n",
      "    BogoMIPS:            4890.91\n",
      "    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mc\n",
      "                         a cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall n\n",
      "                         x mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_go\n",
      "                         od nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl p\n",
      "                         ni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 \n",
      "                         movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_lega\n",
      "                         cy svm extapic cr8_legacy abm sse4a misalignsse 3dnowpr\n",
      "                         efetch osvw ibs skinit wdt tce topoext perfctr_core per\n",
      "                         fctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invp\n",
      "                         cid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall f\n",
      "                         sgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdse\n",
      "                         ed adx smap clflushopt clwb sha_ni xsaveopt xsavec xget\n",
      "                         bv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_\n",
      "                         local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin \n",
      "                         arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean f\n",
      "                         lushbyasid decodeassists pausefilter pfthreshold v_vmsa\n",
      "                         ve_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulq\n",
      "                         dq rdpid overflow_recov succor smca fsrm\n",
      "Virtualization features: \n",
      "  Virtualization:        AMD-V\n",
      "Caches (sum of all):     \n",
      "  L1d:                   4 MiB (128 instances)\n",
      "  L1i:                   4 MiB (128 instances)\n",
      "  L2:                    64 MiB (128 instances)\n",
      "  L3:                    512 MiB (16 instances)\n",
      "NUMA:                    \n",
      "  NUMA node(s):          2\n",
      "  NUMA node0 CPU(s):     0-63\n",
      "  NUMA node1 CPU(s):     64-127\n",
      "Vulnerabilities:         \n",
      "  Itlb multihit:         Not affected\n",
      "  L1tf:                  Not affected\n",
      "  Mds:                   Not affected\n",
      "  Meltdown:              Not affected\n",
      "  Mmio stale data:       Not affected\n",
      "  Retbleed:              Not affected\n",
      "  Spec store bypass:     Mitigation; Speculative Store Bypass disabled via prctl\n",
      "  Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer\n",
      "                          sanitization\n",
      "  Spectre v2:            Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIB\n",
      "                         P disabled, RSB filling, PBRSB-eIBRS Not affected\n",
      "  Srbds:                 Not affected\n",
      "  Tsx async abort:       Not affected\n"
     ]
    }
   ],
   "source": [
    "!lscpu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "_CudaDeviceProperties(name='AMD Instinct MI250X/MI250', major=9, minor=0, gcnArchName='gfx90a:sramecc+:xnack-', total_memory=65520MB, multi_processor_count=104, uuid=63323962-3731-6162-6164-626232366438, L2_cache_size=8MB)\n",
      "_CudaDeviceProperties(name='AMD Instinct MI250X/MI250', major=9, minor=0, gcnArchName='gfx90a:sramecc+:xnack-', total_memory=65520MB, multi_processor_count=104, uuid=61323937-6262-3735-3131-373961323732, L2_cache_size=8MB)\n",
      "_CudaDeviceProperties(name='AMD Instinct MI250X/MI250', major=9, minor=0, gcnArchName='gfx90a:sramecc+:xnack-', total_memory=65520MB, multi_processor_count=104, uuid=36653564-3266-6336-3333-373563623430, L2_cache_size=8MB)\n",
      "_CudaDeviceProperties(name='AMD Instinct MI250X/MI250', major=9, minor=0, gcnArchName='gfx90a:sramecc+:xnack-', total_memory=65520MB, multi_processor_count=104, uuid=36663136-3964-3862-6463-366333336561, L2_cache_size=8MB)\n",
      "_CudaDeviceProperties(name='AMD Instinct MI250X/MI250', major=9, minor=0, gcnArchName='gfx90a:sramecc+:xnack-', total_memory=65520MB, multi_processor_count=104, uuid=63383665-3732-6161-3534-333339316136, L2_cache_size=8MB)\n",
      "_CudaDeviceProperties(name='AMD Instinct MI250X/MI250', major=9, minor=0, gcnArchName='gfx90a:sramecc+:xnack-', total_memory=65520MB, multi_processor_count=104, uuid=31373130-3232-3162-6565-323432383265, L2_cache_size=8MB)\n",
      "_CudaDeviceProperties(name='AMD Instinct MI250X/MI250', major=9, minor=0, gcnArchName='gfx90a:sramecc+:xnack-', total_memory=65520MB, multi_processor_count=104, uuid=37633865-3934-6535-6363-626130623137, L2_cache_size=8MB)\n",
      "_CudaDeviceProperties(name='AMD Instinct MI250X/MI250', major=9, minor=0, gcnArchName='gfx90a:sramecc+:xnack-', total_memory=65520MB, multi_processor_count=104, uuid=64386637-6439-6234-3161-643831623738, L2_cache_size=8MB)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# check enabled GPU\n",
    "print(torch.cuda.is_available())\n",
    "for i in range(torch.cuda.device_count()):\n",
    "   print(torch.cuda.get_device_properties(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.48.1\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from multiprocessing import cpu_count\n",
    "from datasets import load_dataset\n",
    "\n",
    "# set cache directory out of $HOME to $WORK\n",
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"cache/\"\n",
    "default_cache_dir = \"cache/\"\n",
    "\n",
    "import transformers \n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia pages (2023) (Portuguese)\n",
    "\n",
    "- each document (page in wikipedia) is saved in a row \n",
    "\n",
    "- the wikipedia dataset has several columns, including title and etc. I only used the main text of the page\n",
    "\n",
    "- downloading dataset from hugging face only needs to be done once, and it will be saved in cache for future loads!\n",
    "\n",
    "https://huggingface.co/datasets/wikimedia/wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1112246\n",
      "}) \n",
      "\n",
      "Astronomia é uma ciência natural que estuda corpos celestes (como estrelas, planetas, cometas, nebulosas, aglomerados de estrelas, galáxias) e fenômenos que se originam fora da atmosfera da Terra (como a radiação cósmica de fundo em micro-ondas). Preocupada com a evolução, a física e a química de objetos celestes, bem como a formação e o desenvolvimento do universo.\n",
      "\n",
      "A astronomia é uma das mais antigas ciências. Culturas pré-históricas deixaram registrados vários artefatos astronômicos, como Stonehenge, os montes de Newgrange e os menires. As primeiras civilizações, como os babilônios, gregos, chineses, indianos, persas e maias realizaram observações metódicas do céu noturno. No entanto, a invenção do telescópio permitiu o desenvolvimento da astronomia moderna. Historicamente, a astronomia incluiu disciplinas tão diversas como astrometria, navegação astronômica, astronomia observacional e a elaboração de calendários. Durante o período medieval, seu estudo era obrigatório e estava inclu\n"
     ]
    }
   ],
   "source": [
    "wikipedia = load_dataset(\"wikimedia/wikipedia\", \n",
    "                         \"20231101.pt\", \n",
    "                         split=\"train\", \n",
    "                         num_proc=cpu_count(),\n",
    "                         cache_dir=default_cache_dir\n",
    "                         )\n",
    "\n",
    "wikipedia = wikipedia.remove_columns([col for col in wikipedia.column_names if col != \"text\"])  # only keep the 'text' column\n",
    "\n",
    "print(wikipedia, '\\n')\n",
    "\n",
    "print(wikipedia[0][\"text\"][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BrWac \n",
    "\n",
    "https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC\n",
    "\n",
    "The BrWaC (Brazilian Portuguese Web as Corpus) is a large corpus constructed following the Wacky framework,\n",
    "which was made public for research purposes. The current corpus version, released in January 2017, is composed by\n",
    "3.53 million documents, 2.68 billion tokens and 5.79 million types. Please note that this resource is available\n",
    "solely for academic research purposes, and you agreed not to use it for any commercial applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625924f463d74b53a04f9d36c89b31e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 3530796\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brwac = load_dataset(\"brwac\", \n",
    "                    data_dir=\"dataset-brwac\",\n",
    "                    split=\"train\", \n",
    "                    num_proc=cpu_count(),\n",
    "                    cache_dir=default_cache_dir,\n",
    "                    trust_remote_code=True\n",
    "                    )\n",
    "\n",
    "brwac = brwac.remove_columns([col for col in brwac.column_names if col != \"text\"])  # only keep the 'text' column\n",
    "\n",
    "brwac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Conteúdo recente'],\n",
       " ['ESPUMA MARROM CHAMADA \"NINGUÉM MERECE\"'],\n",
       " ['31 de Agosto de 2015, 7:07 , por paulo soavinski - | No one following this article yet.'],\n",
       " ['Visualizado 202 vezes'],\n",
       " ['JORNAL ELETRÔNICO DA ILHA DO MEL'],\n",
       " ['Uma espuma marrom escuro tem aparecido com frequência na Praia de Fora.',\n",
       "  'Na faixa de areia ela aparece disseminada e não chama muito a atenção.',\n",
       "  'No Buraco do Aipo, com muitas pedras, ela aparece concentrada.',\n",
       "  'É fácil saber que esta espuma estranha está lá, quando venta.',\n",
       "  'Pequenos algodões de espuma começam a flutuar no espaço, pertinho da Praia do Saquinho.',\n",
       "  'Quem pode ajudar na coleta deste material, envio a laboratório renomado e pagamento de análises, favor entrar em contato com o site.']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brwac[0][\"text\"][\"paragraphs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteúdo recente\n",
      "ESPUMA MARROM CHAMADA \"NINGUÉM MERECE\"\n",
      "31 de Agosto de 2015, 7:07 , por paulo soavinski - | No one following this article yet.\n",
      "Visualizado 202 vezes\n",
      "JORNAL ELETRÔNICO DA ILHA DO MEL\n",
      "Uma espuma marrom escuro tem aparecido com frequência na Praia de Fora.\n",
      "Na faixa de areia ela aparece disseminada e não chama muito a atenção.\n",
      "No Buraco do Aipo, com muitas pedras, ela aparece concentrada.\n",
      "É fácil saber que esta espuma estranha está lá, quando venta.\n",
      "Pequenos algodões de espuma começam a flutuar no espaço, pertinho da Praia do Saquinho.\n",
      "Quem pode ajudar na coleta deste material, envio a laboratório renomado e pagamento de análises, favor entrar em contato com o site.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from ftfy import fix_text\n",
    "\n",
    "for para in brwac[0][\"text\"][\"paragraphs\"]:\n",
    "    for doc in para:\n",
    "        soup = BeautifulSoup(doc, 'html.parser')\n",
    "        print( fix_text(soup.get_text())  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 3530796\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def para2doc(example):\n",
    "    list_doc = []\n",
    "\n",
    "    for row in example[\"text\"]:\n",
    "        \n",
    "        str_doc = \"\"\n",
    "        \n",
    "        for para in row[\"paragraphs\"]:\n",
    "\n",
    "            for doc in para:\n",
    "                \n",
    "                soup = BeautifulSoup(doc, 'html.parser') \n",
    "\n",
    "                str_doc += fix_text(soup.get_text()) + '\\n'\n",
    "\n",
    "        list_doc.append(str_doc)\n",
    "\n",
    "    return {\"text\" : list_doc}\n",
    "\n",
    "preprocessed_brwac = brwac.map(para2doc,\n",
    "                                batched = True,\n",
    "                                remove_columns=[\"text\"],\n",
    "                                num_proc = cpu_count(),\n",
    "                                )\n",
    "\n",
    "# preprocessed_brwac = preprocessed_brwac.rename_column(\"paragraphs\", \"text\")\n",
    "\n",
    "preprocessed_brwac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Conteúdo recente\\nESPUMA MARROM CHAMADA \"NINGUÉM MERECE\"\\n31 de Agosto de 2015, 7:07 , por paulo soavinski - | No one following this article yet.\\nVisualizado 202 vezes\\nJORNAL ELETRÔNICO DA ILHA DO MEL\\nUma espuma marrom escuro tem aparecido com frequência na Praia de Fora.\\nNa faixa de areia ela aparece disseminada e não chama muito a atenção.\\nNo Buraco do Aipo, com muitas pedras, ela aparece concentrada.\\nÉ fácil saber que esta espuma estranha está lá, quando venta.\\nPequenos algodões de espuma começam a flutuar no espaço, pertinho da Praia do Saquinho.\\nQuem pode ajudar na coleta deste material, envio a laboratório renomado e pagamento de análises, favor entrar em contato com o site.\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_brwac[0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C4 (2020) (Portuguese)\n",
    "\n",
    "https://huggingface.co/datasets/allenai/c4\n",
    "\n",
    "https://paperswithcode.com/dataset/c4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "c4 = load_dataset(  \"allenai/c4\", \n",
    "                    \"pt\", \n",
    "                    split=\"train\", \n",
    "                    num_proc=cpu_count()\n",
    "                    )\n",
    "\n",
    "c4 = c4.remove_columns([col for col in c4.column_names if col != \"text\"])  # only keep the 'text' column\n",
    "\n",
    "print(c4, '\\n')\n",
    "\n",
    "print(c4[0][\"text\"][:1000])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# reduce the c4 huge dataset to 5%, using shuffle \n",
    "\n",
    "c4_10 = c4.train_test_split(train_size=0.05, shuffle=True)\n",
    "\n",
    "c4_10 = c4_10[\"train\"]\n",
    "\n",
    "c4_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oscar\n",
    "\n",
    "- noisy, avoid for now\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "oscar = load_dataset('oscar-corpus/community-oscar', \n",
    "                     data_files='data/2024-38/pt_meta/*.jsonl.zst', \n",
    "                     split='train',\n",
    "                     streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC4 (legal)\n",
    "\n",
    "- multilingual variant of the C4 dataset, composed of natural text drawn from the public Common Crawl Web Scrape.\n",
    "\n",
    "- consider cleaning more this dataset, it is noisy\n",
    "\n",
    "https://huggingface.co/datasets/joelniklaus/legal-mc4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "mc4legal = load_dataset(\"joelito/mc4_legal\", \n",
    "                        \"pt\", \n",
    "                        split='train', \n",
    "                        trust_remote_code=True, \n",
    "                        num_proc=cpu_count())\n",
    "\n",
    "mc4legal = mc4legal.remove_columns([col for col in mc4legal.column_names if col != \"text\"])  # only keep the 'text' column\n",
    "\n",
    "print(mc4legal, '\\n')\n",
    "\n",
    "print(mc4legal[\"text\"][0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi_Legal_Pile (pt)\n",
    "\n",
    "- is a large-scale multilingual legal dataset suited for pretraining language models. It spans over 24 languages and five legal text types.\n",
    "\n",
    "- case_law dataset (Jurisprudência)\n",
    "\n",
    "- very huge dataset, used only 5 %\n",
    "\n",
    "https://huggingface.co/datasets/joelniklaus/Multi_Legal_Pile"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# multilegal has 17M, which is too much, use only 5% (0.86M)\n",
    "multilegal = load_dataset('joelniklaus/Multi_Legal_Pile', \n",
    "                          \"pt_caselaw\", \n",
    "                          split='train[:5%]', \n",
    "                          trust_remote_code=True, \n",
    "                          num_proc=cpu_count()\n",
    "                          )\n",
    "\n",
    "multilegal = multilegal.remove_columns([col for col in multilegal.column_names if col != \"text\"])  # only keep the 'text' column\n",
    "\n",
    "print(multilegal, '\\n')\n",
    "\n",
    "print(multilegal[\"text\"][0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lener-br dataset\n",
    "\n",
    "- LeNER-Br is a dataset for named entity recognition (NER) in Brazilian Legal Text.\n",
    "\n",
    "- Composed of 70 legal documents, manually curated and tagged\n",
    "\n",
    "https://github.com/peluz/lener-br/\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "!wget https://github.com/peluz/lener-br/archive/refs/heads/master.zip\n",
    "\n",
    "!unzip master.zip"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Iterate files and convert the documents to a dataset object\n",
    "\n",
    "raw_directory_path = \"lener-br-master/leNER-Br/raw_text\"\n",
    "\n",
    "raw_lener = { \"text\" : [] }\n",
    "\n",
    "for file in os.listdir(raw_directory_path):\n",
    "    filename = os.fsdecode(file)\n",
    "\n",
    "    if filename.endswith(\".txt\"):\n",
    "\n",
    "        with open(f\"{raw_directory_path}/{filename}\", 'r') as f:\n",
    "            raw_lener[\"text\"].append( f.read() )\n",
    "\n",
    "lener = Dataset.from_dict(raw_lener)\n",
    "\n",
    "print(lener, '\\n')\n",
    "\n",
    "print(lener[\"text\"][0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate Datasets \n",
    "\n",
    "- skip mc4 for now (noisy)\n",
    "\n",
    "- lener is very small compared with the others\n",
    "\n",
    "- complete dataset is around 50% wikipedia, 50% legal docs\n",
    "\n",
    "- 1.97 M documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 4643042\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# raw_datasets = concatenate_datasets([wikipedia, brwac])\n",
    "raw_datasets = concatenate_datasets([wikipedia, preprocessed_brwac])\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'à',\n",
       " 'ao',\n",
       " 'aos',\n",
       " 'aquela',\n",
       " 'aquelas',\n",
       " 'aquele',\n",
       " 'aqueles',\n",
       " 'aquilo',\n",
       " 'as',\n",
       " 'às',\n",
       " 'até',\n",
       " 'com',\n",
       " 'como',\n",
       " 'da',\n",
       " 'das',\n",
       " 'de',\n",
       " 'dela',\n",
       " 'delas',\n",
       " 'dele',\n",
       " 'deles',\n",
       " 'depois',\n",
       " 'do',\n",
       " 'dos',\n",
       " 'e',\n",
       " 'é',\n",
       " 'ela',\n",
       " 'elas',\n",
       " 'ele',\n",
       " 'eles',\n",
       " 'em',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'eram',\n",
       " 'éramos',\n",
       " 'essa',\n",
       " 'essas',\n",
       " 'esse',\n",
       " 'esses',\n",
       " 'esta',\n",
       " 'está',\n",
       " 'estamos',\n",
       " 'estão',\n",
       " 'estar',\n",
       " 'estas',\n",
       " 'estava',\n",
       " 'estavam',\n",
       " 'estávamos',\n",
       " 'este',\n",
       " 'esteja',\n",
       " 'estejam',\n",
       " 'estejamos',\n",
       " 'estes',\n",
       " 'esteve',\n",
       " 'estive',\n",
       " 'estivemos',\n",
       " 'estiver',\n",
       " 'estivera',\n",
       " 'estiveram',\n",
       " 'estivéramos',\n",
       " 'estiverem',\n",
       " 'estivermos',\n",
       " 'estivesse',\n",
       " 'estivessem',\n",
       " 'estivéssemos',\n",
       " 'estou',\n",
       " 'eu',\n",
       " 'foi',\n",
       " 'fomos',\n",
       " 'for',\n",
       " 'fora',\n",
       " 'foram',\n",
       " 'fôramos',\n",
       " 'forem',\n",
       " 'formos',\n",
       " 'fosse',\n",
       " 'fossem',\n",
       " 'fôssemos',\n",
       " 'fui',\n",
       " 'há',\n",
       " 'haja',\n",
       " 'hajam',\n",
       " 'hajamos',\n",
       " 'hão',\n",
       " 'havemos',\n",
       " 'haver',\n",
       " 'hei',\n",
       " 'houve',\n",
       " 'houvemos',\n",
       " 'houver',\n",
       " 'houvera',\n",
       " 'houverá',\n",
       " 'houveram',\n",
       " 'houvéramos',\n",
       " 'houverão',\n",
       " 'houverei',\n",
       " 'houverem',\n",
       " 'houveremos',\n",
       " 'houveria',\n",
       " 'houveriam',\n",
       " 'houveríamos',\n",
       " 'houvermos',\n",
       " 'houvesse',\n",
       " 'houvessem',\n",
       " 'houvéssemos',\n",
       " 'isso',\n",
       " 'isto',\n",
       " 'já',\n",
       " 'lhe',\n",
       " 'lhes',\n",
       " 'mais',\n",
       " 'mas',\n",
       " 'me',\n",
       " 'mesmo',\n",
       " 'meu',\n",
       " 'meus',\n",
       " 'minha',\n",
       " 'minhas',\n",
       " 'muito',\n",
       " 'na',\n",
       " 'não',\n",
       " 'nas',\n",
       " 'nem',\n",
       " 'no',\n",
       " 'nos',\n",
       " 'nós',\n",
       " 'nossa',\n",
       " 'nossas',\n",
       " 'nosso',\n",
       " 'nossos',\n",
       " 'num',\n",
       " 'numa',\n",
       " 'o',\n",
       " 'os',\n",
       " 'ou',\n",
       " 'para',\n",
       " 'pela',\n",
       " 'pelas',\n",
       " 'pelo',\n",
       " 'pelos',\n",
       " 'por',\n",
       " 'qual',\n",
       " 'quando',\n",
       " 'que',\n",
       " 'quem',\n",
       " 'são',\n",
       " 'se',\n",
       " 'seja',\n",
       " 'sejam',\n",
       " 'sejamos',\n",
       " 'sem',\n",
       " 'ser',\n",
       " 'será',\n",
       " 'serão',\n",
       " 'serei',\n",
       " 'seremos',\n",
       " 'seria',\n",
       " 'seriam',\n",
       " 'seríamos',\n",
       " 'seu',\n",
       " 'seus',\n",
       " 'só',\n",
       " 'somos',\n",
       " 'sou',\n",
       " 'sua',\n",
       " 'suas',\n",
       " 'também',\n",
       " 'te',\n",
       " 'tem',\n",
       " 'tém',\n",
       " 'temos',\n",
       " 'tenha',\n",
       " 'tenham',\n",
       " 'tenhamos',\n",
       " 'tenho',\n",
       " 'terá',\n",
       " 'terão',\n",
       " 'terei',\n",
       " 'teremos',\n",
       " 'teria',\n",
       " 'teriam',\n",
       " 'teríamos',\n",
       " 'teu',\n",
       " 'teus',\n",
       " 'teve',\n",
       " 'tinha',\n",
       " 'tinham',\n",
       " 'tínhamos',\n",
       " 'tive',\n",
       " 'tivemos',\n",
       " 'tiver',\n",
       " 'tivera',\n",
       " 'tiveram',\n",
       " 'tivéramos',\n",
       " 'tiverem',\n",
       " 'tivermos',\n",
       " 'tivesse',\n",
       " 'tivessem',\n",
       " 'tivéssemos',\n",
       " 'tu',\n",
       " 'tua',\n",
       " 'tuas',\n",
       " 'um',\n",
       " 'uma',\n",
       " 'você',\n",
       " 'vocês',\n",
       " 'vos']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Dataset documents by their paragraphs, to generate more rows\n",
    "\n",
    "Each page will be split by their paragraphs, each one becoming a new document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'num_words', 'stopwords', 'average'],\n",
       "    num_rows: 156783923\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analize_texts(row):\n",
    "\n",
    "    list_para = []\n",
    "    list_words = []\n",
    "    list_stopwords = []\n",
    "    list_average = []\n",
    "\n",
    "    for doc in row[\"text\"]:\n",
    "        for paragraph in doc.split('\\n'):\n",
    "\n",
    "            # strip whitespaces\n",
    "            paragraph = paragraph.strip()\n",
    "\n",
    "            # skip single or empty worded paragraphs\n",
    "            if (len(paragraph.split()) < 2):\n",
    "                continue\n",
    "\n",
    "            # count how many stopwords are in the paragraph\n",
    "            stopwords_cnt = 0    \n",
    "            for word in paragraph.split():\n",
    "                for stop in stopwords:\n",
    "                    if stop.casefold() == word.casefold():  # insensitive case\n",
    "                        stopwords_cnt += 1\n",
    "                        break # count once and speed up everything\n",
    "            \n",
    "            # count non whitespace characters\n",
    "            characters = 0\n",
    "            for word in paragraph.split():\n",
    "                characters += len(word)\n",
    "\n",
    "            list_para.append(paragraph)\n",
    "            list_words.append(len(paragraph.split()))\n",
    "            list_stopwords.append(stopwords_cnt)\n",
    "            list_average.append(characters/len(paragraph.split()))\n",
    "\n",
    "    return {\"paragraphs\" : list_para, \"num_words\" : list_words, \"stopwords\" : list_stopwords, \"average\" : list_average}\n",
    "\n",
    "preprocessed_datasets = raw_datasets.map(analize_texts,\n",
    "                                         batched = True,\n",
    "                                         remove_columns=[\"text\"],\n",
    "                                         num_proc = cpu_count(),\n",
    "                                        )\n",
    "\n",
    "preprocessed_datasets = preprocessed_datasets.rename_column(\"paragraphs\", \"text\")\n",
    "\n",
    "preprocessed_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a16db3428abd408b8cf758a86a7a78f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/49 shards):   0%|          | 0/156783923 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed_datasets.save_to_disk(\"dataset/para-num-stop-avg\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# check if dataset is correctly separated by paragraphs\n",
    "preprocessed_datasets[:10][\"text\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "preprocessed_datasets[-10:][\"text\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "preprocessed_datasets[:10][\"num_words\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "preprocessed_datasets[-10:][\"stopwords\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "preprocessed_datasets[-10:][\"average\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "values = preprocessed_datasets[\"num_words\"]\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.hist(values, bins=40, edgecolor='black', log=True)\n",
    "plt.title(f\"Distribution of num_words\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "min_val, max_val = min(values), max(values)\n",
    "x_ticks = np.linspace(min_val, max_val, num=40)  # 10 evenly spaced ticks\n",
    "plt.xticks(x_ticks, rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove documents with more than 512 words (will be truncated and lose meaning)\n",
    "\n",
    "- also, the relative amount is insignificant\n",
    "\n",
    "----\n",
    "\n",
    "- Remove text that has less than 20 words. \n",
    "\n",
    "- This will save up a lot of computational effort, while keeping learning still good"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "preprocessed_datasets2 = preprocessed_datasets.filter(\n",
    "    lambda example: \n",
    "            example[\"num_words\"] >= 20 \n",
    "        and example[\"num_words\"] <= 512\n",
    "        and example[\"stopwords\"] >= 1\n",
    "        and example[\"average\"] >= 2 \n",
    "        and example[\"average\"] <= 15\n",
    "    )\n",
    "\n",
    "preprocessed_datasets2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "values = preprocessed_datasets2[\"num_words\"]\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.hist(values, bins=40, edgecolor='black', log=True)\n",
    "plt.title(f\"Distribution of num_words\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "min_val, max_val = min(values), max(values)\n",
    "x_ticks = np.linspace(min_val, max_val, num=40)  # 40 evenly spaced ticks\n",
    "plt.xticks(x_ticks, rotation=45)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# remove the num_words column\n",
    "preprocessed_datasets2 = preprocessed_datasets2.remove_columns([col for col in preprocessed_datasets2.column_names if col != \"text\"])  # only keep the 'text' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_name = \"dataset/split-by-paragraph\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "preprocessed_datasets2.save_to_disk(preprocessed_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a2dcb43b7148308c9c28500db1baf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "preprocessed_datasets2 = load_from_disk(preprocessed_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate dataset into train and test\n",
    "\n",
    "*The splits are shuffled by default, but you can set shuffle=False to prevent shuffling.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 56726693\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 6302966\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset = preprocessed_datasets2.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "\n",
    "split_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_name_dataset = \"dataset/split-by-paragraph-train-test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "split_dataset.save_to_disk(split_name_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb406515d2b4130a3a9748ee25b4c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "split_dataset = load_from_disk(split_name_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ao final de maio deste ano, quase 100 pessoas foram detidas numa investida global contra os criadores, vendedores e usuários do Blackshades RAT.',\n",
       " 'Para celebrar este acontecimento, realizou-se uma Celebração Eucarística na Catedral Metropolitana de Pelotas com a presença de todas as crianças, familiares, educadores, voluntários, irmãs, diretoria, amigos e amigas que vieram celebrar conosco.',\n",
       " '- (OFF) O Rei continua.Assim Sagínero Jeiper representa o começo da vida do ser que queira conquistar o Auto-equilibrio da Vida Espiritual até mesmo antes de iniciar-se como um discípulo de um mestre físico.- E assim Sagínero se expõe com sua consciência dizendo com ênfase.',\n",
       " 'Apesar de a identidade do suspeito ser conhecida, a Polícia pediu sigilo até a divulgação do resultado da perícia efetuada no computador do suspeito.',\n",
       " 'Na manhã do dia 25 de Setembro de 1968, Américo Boavida foi vitimado por um bombardeamento aéreo do exército português à \"Base Hanói II\" do MPLA, onde se encontrava, perto do rio Luati e da floresta de Cambule, no Moxico.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset[\"train\"][:5][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mais do que isso, é a intenção da Sete Engenharia, que executa obras no local, próximo à Praça da Bíblia.',\n",
       " 'Ele gostou da idéia, mas desistiu assim que um colega disse que era estranho, pois só pintava os desenhos de preto.',\n",
       " 'O objetivo geral deste trabalho foi verificar quais os procedimentos adotados para a indexação da informação jurídica e como isso poderia ser refletido em uma política de indexação.',\n",
       " 'Não encontro palavras que expressem o significado em minha vida do aprendizado do canto orfeônico e da participação em corais, desde criança até a universidade (fui a primeira inscrita no Coral da UFMA).',\n",
       " 'Finalmente, o Papa Bento XVI invoca a bênção de Deus sobre a FAO e sublinha que \"a Igreja está sempre pronta para trabalhar pela derrota da fome.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset[\"test\"][:5][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 32_768\n",
    "context_size = 512\n",
    "tokenizer_name = f\"tokenizers/custom/{vocabulary_size:_}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a tokenizer, from scratch! \n",
    "\n",
    "- it is important to train a tokenizer, because it is the responsible for representing the input data so the model is able to interact and interpret the vocabulary provided!\n",
    "\n",
    "- each word used in the input, should have a token (or a sequence of tokens) that will be used to represent it, and become embeddings when interacting with the model\n",
    "\n",
    "- the pre-loaded tokenizer is uncased, meaning that all uppercase letters will be converted to lowercase to reduce complexity and vocabulary size\n",
    "\n",
    "It will contain the following special tokens:\n",
    "\n",
    "    0. [PAD]: Padding (to fill empty spots)\n",
    "    1. [UNK]: Unknown\n",
    "    2. [CLS]: Classification (initial token used as classifier)\n",
    "    3. [SEP]: Separator (for sequences) (and also it is the ending token)\n",
    "    4. [MASK]: Masking (token that represents a masked token)\n",
    "\n",
    "- It's important for a tokenizer to have a **good compression rate**, meaning that the sequence of tokens generated will be as small as possible. As we know that is computationally expensive to increase the context size (sequence token limit)\n",
    "\n",
    "- Vocabulary size will help with the compression rate (low fertility), the higher the vocab size, the lower the need for splitting words into subtokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specials Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordered by the index\n",
    "custom_special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xc3\\x85' b'\\xe2\\x84\\xab'\n",
      "b'oi, \\xc3\\xa5 \\xc3\\xa5 tudo bem? o \"qu\\xc3\\xaa\" \"essa\" \"fun\\xc3\\xa7\\xc3\\xa3o\" faz?'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'oi, å å tudo bem? o \"quê\" \"essa\" \"função\" faz?'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers.normalizers import NFC, Lowercase, Replace\n",
    "from tokenizers import normalizers\n",
    "\n",
    "# replace latex usage of \"aspas\"\n",
    "# NFC - Canonico (evita equivalencia) e composto, menos characteres\n",
    "# lowercase - unifica maiuscula e mi nusculo, que geralmente muda pouco (o positional embedding pode cuidar de comecar com maiuscula)\n",
    "\n",
    "# NFC, NFK\n",
    "# Ã A~\n",
    "\n",
    "custom_normalizer = normalizers.Sequence([\n",
    "    Replace(\"``\", '\"'), \n",
    "    Replace(\"''\", '\"'),\n",
    "    NFC(), \n",
    "    Lowercase(),\n",
    "])\n",
    "\n",
    "print(\"Å\".encode(\"utf-8\"), \"Å\".encode(\"utf-8\"))\n",
    "\n",
    "normalized_sample = custom_normalizer.normalize_str(\"Oi, Å Å tudo BEM? O ``quê'' \\\"essa\\\" ''função'' faz?\")\n",
    "\n",
    "print(normalized_sample.encode(\"utf-8\"))\n",
    "\n",
    "normalized_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁Quer', (0, 4)),\n",
       " ('▁saber', (5, 10)),\n",
       " ('▁,', (10, 11)),\n",
       " ('▁vamos', (12, 17)),\n",
       " ('▁ver', (18, 21)),\n",
       " ('▁o', (22, 23)),\n",
       " ('▁que', (24, 27)),\n",
       " ('▁esse', (28, 32)),\n",
       " ('▁pre', (33, 36)),\n",
       " ('▁-', (36, 37)),\n",
       " ('▁tokenizer', (37, 46)),\n",
       " ('▁faz', (47, 50)),\n",
       " ('▁com', (51, 54)),\n",
       " ('▁numeros', (55, 62)),\n",
       " ('▁:', (62, 63)),\n",
       " ('▁12345', (64, 69)),\n",
       " ('▁123', (70, 73)),\n",
       " ('▁192', (74, 77)),\n",
       " ('▁193', (78, 81)),\n",
       " ('▁!', (81, 82)),\n",
       " ('▁?', (82, 83)),\n",
       " ('▁!', (83, 84))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import pre_tokenizers\n",
    "from tokenizers.pre_tokenizers import Punctuation, Metaspace, WhitespaceSplit, Digits\n",
    "# to keep the original words (separed by whitespaces), we use metaspaces, represented by:\n",
    "# “▁” U+2581 Lower One Eighth Block Unicode Character\n",
    "\n",
    "# Digits(individual_digits=True), Whitespace()\n",
    "\n",
    "custom_pre_tokenizer = pre_tokenizers.Sequence([WhitespaceSplit(), Punctuation(), Digits(individual_digits=False), Metaspace(replacement=\"▁\", prepend_scheme=\"always\")])\n",
    "\n",
    "custom_pre_tokenizer.pre_tokenize_str(\"Quer saber, vamos ver o que esse pre-tokenizer faz com numeros: 12345 123 192 193!?!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁o', (0, 1)),\n",
       " ('▁quê', (2, 5)),\n",
       " ('▁ele', (6, 9)),\n",
       " ('▁faz', (10, 13)),\n",
       " ('▁com', (16, 19)),\n",
       " ('▁espaços', (28, 35)),\n",
       " ('▁adicionais', (36, 46)),\n",
       " ('▁e', (50, 51)),\n",
       " ('▁quebras', (52, 59)),\n",
       " ('▁de', (60, 62)),\n",
       " ('▁linhas', (63, 69)),\n",
       " ('▁?', (69, 70))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_pre_tokenizer.pre_tokenize_str(\"o quê ele faz \\t com         espaços adicionais \\n\\n e quebras de linhas?   \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "custom_post_processor = TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", custom_special_tokens.index(\"[CLS]\")), (\"[SEP]\", custom_special_tokens.index(\"[SEP]\"))],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.decoders import Metaspace\n",
    "\n",
    "custom_decoder = Metaspace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train tokenizer with unigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import Unigram\n",
    "\n",
    "custom_tokenizer = Tokenizer(Unigram())\n",
    "\n",
    "custom_tokenizer.normalizer = custom_normalizer\n",
    "custom_tokenizer.pre_tokenizer = custom_pre_tokenizer\n",
    "custom_tokenizer.post_processor = custom_post_processor\n",
    "custom_tokenizer.decoder = custom_decoder"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from tokenizers.trainers import UnigramTrainer\n",
    "\n",
    "custom_trainer = UnigramTrainer(\n",
    "        vocab_size=vocabulary_size,\n",
    "        initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
    "        special_tokens=custom_special_tokens,\n",
    "        unk_token=\"[UNK]\",\n",
    ")\n",
    "\n",
    "# create a python generator to dynamically load the data, one batch at a time\n",
    "def batch_iterator(batch_size=128): # 128 (cores)\n",
    "    for i in tqdm(range(0, len(split_dataset[\"train\"]), batch_size)):\n",
    "        yield split_dataset[\"train\"][i : i + batch_size][\"text\"]\n",
    "\n",
    "custom_tokenizer.train_from_iterator(iterator=batch_iterator(), trainer=custom_trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to a fast instance of tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "fast_custom_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=custom_tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    bos_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    eos_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    "    padding_side=\"right\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the tokenizer: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# save locally\n",
    "fast_custom_tokenizer.save_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='tokenizers/custom/32_768', vocab_size=32768, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, \n",
    "                                          local_files_only=True, \n",
    "                                          cache_dir = default_cache_dir\n",
    "                                          )\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁olá', '▁pessoal', '▁', ',', '▁como', '▁vocês', '▁estão', '▁', '😁', '▁', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# print( tokenizer_albert.convert_ids_to_tokens( tokenizer_albert.encode(\"Olá pessoal, Como vocês estão 😁 ?\") ) )\n",
    "\n",
    "print( tokenizer.convert_ids_to_tokens( tokenizer.encode(\"Olá pessoal, Como vocês estão 😁 ?\") ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 5, 7, 16208, 5, 8, 2535, 24, 5553, 5, 6, 47, 5, 8, 5553, 32, 2151, 3]\n",
      "['[CLS]', '▁', 'a', 'tiraram', '▁', 'o', '▁pau', '▁no', '▁gato', '▁', ',', '▁mas', '▁', 'o', '▁gato', '▁não', '▁morreu', '[SEP]']\n",
      "\n",
      "[2, 32, 774, 5, 6, 104, 774, 13, 43, 113, 5, 12, 5, 12, 5, 12, 3]\n",
      "['[CLS]', '▁não', '▁sei', '▁', ',', '▁só', '▁sei', '▁que', '▁foi', '▁assim', '▁', '.', '▁', '.', '▁', '.', '[SEP]']\n",
      "\n",
      "[2, 5, 16436, 5, 8, 427, 12153, 5, 6, 5, 11, 6102, 5, 8, 427, 587, 10929, 1272, 5, 31, 13588, 5, 30, 3]\n",
      "['[CLS]', '▁', 'testando', '▁', 'o', '▁modo', '▁continuo', '▁', ',', '▁', 'e', '▁tambem', '▁', 'o', '▁modo', '▁sub', 'jun', 'tivo', '▁', '(', '▁soubesse', '▁', ')', '[SEP]']\n",
      "\n",
      "[2, 3563, 5, 6, 4058, 5, 6, 378, 5, 6, 13011, 5, 6, 5, 19156, 5, 6, 2229, 5, 6, 480, 3]\n",
      "['[CLS]', '▁justo', '▁', ',', '▁justa', '▁', ',', '▁justiça', '▁', ',', '▁injusto', '▁', ',', '▁', 'injustamente', '▁', ',', '▁justamente', '▁', ',', '▁junto', '[SEP]']\n",
      "\n",
      "[2, 5, 16436, 5, 7, 2211, 8, 10, 5, 6, 178, 13, 644, 5, 8, 10, 5, 7, 2211, 8, 10, 161, 7, 5, 7, 5, 7, 2274, 9503, 14, 189, 535, 5, 101, 3]\n",
      "['[CLS]', '▁', 'testando', '▁', 'a', 'cent', 'o', 's', '▁', ',', '▁será', '▁que', '▁manter', '▁', 'o', 's', '▁', 'a', 'cent', 'o', 's', '▁melhor', 'a', '▁', 'a', '▁', 'a', 'cur', 'ácia', '▁do', '▁meu', '▁modelo', '▁', '?', '[SEP]']\n",
      "\n",
      "[2, 1008, 5, 50, 5, 7, 23, 34, 9678, 5, 6, 2915, 5, 6, 5, 7, 23, 34, 13257, 5, 6, 5, 7, 5185, 158, 5, 6, 2915, 780, 8, 5, 6, 2915, 2326, 780, 8, 3]\n",
      "['[CLS]', '▁amigo', '▁', ':', '▁', 'a', 'm', 'i', 'guinho', '▁', ',', '▁amiga', '▁', ',', '▁', 'a', 'm', 'i', 'guinha', '▁', ',', '▁', 'a', 'mig', 'ão', '▁', ',', '▁amiga', 'ç', 'o', '▁', ',', '▁amiga', 'lha', 'ç', 'o', '[SEP]']\n",
      "\n",
      "[2, 13599, 5, 25, 29, 1009, 820, 13, 4106, 4310, 5, 21767, 5, 31, 36, 2936, 5, 6, 1558, 10, 5, 6, 13428, 10, 5, 6, 5, 19696, 10, 5, 6, 5, 14962, 10, 9, 2936, 5, 6, 8319, 10, 5, 30, 5, 11, 6255, 13, 26, 8309, 23, 343, 15, 4544, 15, 367, 5, 31, 36, 5, 7, 7331, 16437, 9, 795, 16, 1811, 5, 20, 4436, 5, 30, 5, 12, 9883, 19, 5, 7, 1645, 5, 6, 5, 7, 932, 5, 11, 5, 7, 2981, 9, 2042, 5, 21767, 5, 6, 87, 36, 5, 7, 494, 5, 11, 5, 8, 247, 14, 1477, 5, 12, 3]\n",
      "['[CLS]', '▁astronomia', '▁', 'é', '▁uma', '▁ciência', '▁natural', '▁que', '▁estuda', '▁corpos', '▁', 'celestes', '▁', '(', '▁como', '▁estrelas', '▁', ',', '▁planeta', 's', '▁', ',', '▁cometa', 's', '▁', ',', '▁', 'nebulosa', 's', '▁', ',', '▁', 'aglomerado', 's', '▁de', '▁estrelas', '▁', ',', '▁galáxia', 's', '▁', ')', '▁', 'e', '▁fenômenos', '▁que', '▁se', '▁origina', 'm', '▁fora', '▁da', '▁atmosfera', '▁da', '▁terra', '▁', '(', '▁como', '▁', 'a', '▁radiação', '▁cósmica', '▁de', '▁fundo', '▁em', '▁micro', '▁', '-', '▁ondas', '▁', ')', '▁', '.', '▁preocupada', '▁com', '▁', 'a', '▁evolução', '▁', ',', '▁', 'a', '▁física', '▁', 'e', '▁', 'a', '▁química', '▁de', '▁objetos', '▁', 'celestes', '▁', ',', '▁bem', '▁como', '▁', 'a', '▁formação', '▁', 'e', '▁', 'o', '▁desenvolvimento', '▁do', '▁universo', '▁', '.', '[SEP]']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_tokenizer(sample):\n",
    "    encoding = tokenizer.encode(sample)\n",
    "    print(encoding)\n",
    "    print(tokenizer.convert_ids_to_tokens(encoding))\n",
    "    print()\n",
    "\n",
    "test_tokenizer('''\n",
    "Atiraram o pau no gato, mas o gato não morreu\n",
    "''')\n",
    "\n",
    "test_tokenizer('''\n",
    "Não sei, só sei que foi assim...\n",
    "''')\n",
    "\n",
    "test_tokenizer('''\n",
    "Testando o modo continuo, e tambem o modo subjuntivo ( soubesse )\n",
    "''')\n",
    "\n",
    "test_tokenizer('''\n",
    "justo, justa, justiça, injusto, injustamente, justamente, junto\n",
    "''')\n",
    "\n",
    "test_tokenizer('''\n",
    "    testando acentos, será que manter os acentos melhora a acurácia do meu modelo? \n",
    "''')\n",
    "\n",
    "test_tokenizer('''\n",
    "    amigo: amiguinho, amiga, amiguinha, amigão, amigaço, amigalhaço\n",
    "''')\n",
    "\n",
    "test_tokenizer(\n",
    "    preprocessed_datasets2[0][\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objetively Evaluate Tokenizer on Compression Rate (1/Fertility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁ao', (0, 2)),\n",
       " ('▁final', (3, 8)),\n",
       " ('▁de', (9, 11)),\n",
       " ('▁maio', (12, 16)),\n",
       " ('▁deste', (17, 22)),\n",
       " ('▁ano', (23, 26)),\n",
       " ('▁,', (26, 27)),\n",
       " ('▁quase', (28, 33)),\n",
       " ('▁100', (34, 37)),\n",
       " ('▁pessoas', (38, 45)),\n",
       " ('▁foram', (46, 51)),\n",
       " ('▁detidas', (52, 59)),\n",
       " ('▁numa', (60, 64)),\n",
       " ('▁investida', (65, 74)),\n",
       " ('▁global', (75, 81)),\n",
       " ('▁contra', (82, 88)),\n",
       " ('▁os', (89, 91)),\n",
       " ('▁criadores', (92, 101)),\n",
       " ('▁,', (101, 102)),\n",
       " ('▁vendedores', (103, 113)),\n",
       " ('▁e', (114, 115)),\n",
       " ('▁usuários', (116, 124)),\n",
       " ('▁do', (125, 127)),\n",
       " ('▁blackshades', (128, 139)),\n",
       " ('▁rat', (140, 143)),\n",
       " ('▁.', (143, 144))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_and_pre_tokenize(text):\n",
    "    normalized = tokenizer.backend_tokenizer.normalizer.normalize_str(text)\n",
    "    processed = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(normalized)\n",
    "    return processed\n",
    "\n",
    "normalize_and_pre_tokenize( split_dataset[\"train\"][0][\"text\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(batch):\n",
    "    \n",
    "    original_tokens = 0\n",
    "    generated_tokens = 0\n",
    "\n",
    "    for doc in batch[\"text\"]:\n",
    "\n",
    "        original_tokens += len( normalize_and_pre_tokenize(doc) )\n",
    "        generated_tokens += len( tokenizer.encode(doc) )\n",
    "        \n",
    "    # Add the token counts as a new column to the batch\n",
    "    return {\n",
    "        \"generated\": [generated_tokens],\n",
    "        \"original\": [original_tokens]\n",
    "    }\n",
    "\n",
    "evaluate_fertility = split_dataset[\"train\"].map(count_tokens, \n",
    "                                      batched=True,\n",
    "                                      remove_columns=[\"text\"], \n",
    "                                      num_proc=cpu_count()\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3668402771 2388707891\n",
      "fertility: 1.5357268190143891\n"
     ]
    }
   ],
   "source": [
    "total_generated = sum(evaluate_fertility[\"generated\"])\n",
    "total_original = sum(evaluate_fertility[\"original\"])\n",
    "\n",
    "print(total_generated, total_original)\n",
    "\n",
    "print(\"fertility:\", total_generated/total_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset/tokenized-for-training/custom/vocab_size:32_768/context_size:512'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets_name = f\"dataset/tokenized-for-training/custom/vocab_size:{vocabulary_size:_}/context_size:{context_size}\"\n",
    "\n",
    "tokenized_datasets_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bert-base-uncased tokenizer was configured to use a {model_max_length} of 512, meaning that the sequence input given to the bert model can have up to 512 tokens (context size).\n",
    "\n",
    "Nevertheless, for better performance (speed), I will hard limit if to 128:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='tokenizers/custom/32_768', vocab_size=32768, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length = context_size\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'])\n"
     ]
    }
   ],
   "source": [
    "tokenizer_example = tokenizer(\n",
    "    preprocessed_datasets2[0][\"text\"],\n",
    "    return_tensors=\"pt\", # pt = pytorch\n",
    "    max_length=context_size,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_special_tokens_mask=True,\n",
    ")\n",
    "\n",
    "print(tokenizer_example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '▁astronomia',\n",
       " '▁',\n",
       " 'é',\n",
       " '▁uma',\n",
       " '▁ciência',\n",
       " '▁natural',\n",
       " '▁que',\n",
       " '▁estuda',\n",
       " '▁corpos',\n",
       " '▁',\n",
       " 'celestes',\n",
       " '▁',\n",
       " '(',\n",
       " '▁como',\n",
       " '▁estrelas',\n",
       " '▁',\n",
       " ',',\n",
       " '▁planeta',\n",
       " 's',\n",
       " '▁',\n",
       " ',',\n",
       " '▁cometa',\n",
       " 's',\n",
       " '▁',\n",
       " ',',\n",
       " '▁',\n",
       " 'nebulosa',\n",
       " 's',\n",
       " '▁',\n",
       " ',',\n",
       " '▁',\n",
       " 'aglomerado',\n",
       " 's',\n",
       " '▁de',\n",
       " '▁estrelas',\n",
       " '▁',\n",
       " ',',\n",
       " '▁galáxia',\n",
       " 's',\n",
       " '▁',\n",
       " ')',\n",
       " '▁',\n",
       " 'e',\n",
       " '▁fenômenos',\n",
       " '▁que',\n",
       " '▁se',\n",
       " '▁origina',\n",
       " 'm',\n",
       " '▁fora',\n",
       " '▁da',\n",
       " '▁atmosfera',\n",
       " '▁da',\n",
       " '▁terra',\n",
       " '▁',\n",
       " '(',\n",
       " '▁como',\n",
       " '▁',\n",
       " 'a',\n",
       " '▁radiação',\n",
       " '▁cósmica',\n",
       " '▁de',\n",
       " '▁fundo',\n",
       " '▁em',\n",
       " '▁micro',\n",
       " '▁',\n",
       " '-',\n",
       " '▁ondas',\n",
       " '▁',\n",
       " ')',\n",
       " '▁',\n",
       " '.',\n",
       " '▁preocupada',\n",
       " '▁com',\n",
       " '▁',\n",
       " 'a',\n",
       " '▁evolução',\n",
       " '▁',\n",
       " ',',\n",
       " '▁',\n",
       " 'a',\n",
       " '▁física',\n",
       " '▁',\n",
       " 'e',\n",
       " '▁',\n",
       " 'a',\n",
       " '▁química',\n",
       " '▁de',\n",
       " '▁objetos',\n",
       " '▁',\n",
       " 'celestes',\n",
       " '▁',\n",
       " ',',\n",
       " '▁bem',\n",
       " '▁como',\n",
       " '▁',\n",
       " 'a',\n",
       " '▁formação',\n",
       " '▁',\n",
       " 'e',\n",
       " '▁',\n",
       " 'o',\n",
       " '▁desenvolvimento',\n",
       " '▁do',\n",
       " '▁universo',\n",
       " '▁',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens( tokenizer_example[\"input_ids\"].tolist()[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2, 13599,     5,    25,    29,  1009,   820,    13,  4106,  4310,\n",
       "             5, 21767,     5,    31,    36,  2936,     5,     6,  1558,    10,\n",
       "             5,     6, 13428,    10,     5,     6,     5, 19696,    10,     5,\n",
       "             6,     5, 14962,    10,     9,  2936,     5,     6,  8319,    10,\n",
       "             5,    30,     5,    11,  6255,    13,    26,  8309,    23,   343,\n",
       "            15,  4544,    15,   367,     5,    31,    36,     5,     7,  7331,\n",
       "         16437,     9,   795,    16,  1811,     5,    20,  4436,     5,    30,\n",
       "             5,    12,  9883,    19,     5,     7,  1645,     5,     6,     5,\n",
       "             7,   932,     5,    11,     5,     7,  2981,     9,  2042,     5,\n",
       "         21767,     5,     6,    87,    36,     5,     7,   494,     5,    11,\n",
       "             5,     8,   247,    14,  1477,     5,    12,     3,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_example[\"input_ids\"] # the id of the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_example[\"token_type_ids\"] # 0 for sentenceA and special tokens, 1 for sentenceB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_example[\"attention_mask\"] # 0 for tokens that don't interfere in the attention computation (value matrix), 1 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_example[\"special_tokens_mask\"] # 1 for special tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the Dataset:\n",
    "\n",
    "This next step is to manipulate the dataset for a suitable format for the model:\n",
    "\n",
    "- That is, tokenize the dataset, and truncate the documents with more than {model_max_length} tokens. \n",
    "\n",
    "- This is done in the CPU in batches and parellized across the cores."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "print(f\"The tokenizer will keep only: {context_size} tokens\" )\n",
    "\n",
    "def group_texts(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"text\"], \n",
    "        max_length=context_size,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "# preprocess dataset\n",
    "\n",
    "tokenized_datasets = split_dataset.map(group_texts, \n",
    "                                      batched=True,\n",
    "                                      remove_columns=[\"text\"], \n",
    "                                      num_proc=cpu_count()\n",
    "                                      )\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dataset, now, already tokenized, locally. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# save tokenized dataset locally:\n",
    "tokenized_datasets.save_to_disk(tokenized_datasets_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenized Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fafac298c149448cb729ccb62e987978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dad18077efe4f8bb7fb08be3fd4c6bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'special_tokens_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)} \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 56726693\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 6302966\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# load tokenized dataset locally\n",
    "tokenized_datasets = load_from_disk(tokenized_datasets_name)\n",
    "\n",
    "print(tokenized_datasets[\"train\"].features, '\\n')\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "filtered_datasets_name = f\"dataset/filtered/custom/vocab_size:{vocabulary_size:_}/context_size:{context_size}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the bert model faster, we will optimize the documents used for training, that is, we will reduce the dataset to retain only good documents. \n",
    "\n",
    "**The criteria used**  *the documents will be filtered and only be valid when*:\n",
    "\n",
    "    - it has more than 32 tokens.\n",
    "\n",
    "    - it was not truncated"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "print(\"first quartile:\")\n",
    "\n",
    "for elem in tokenized_datasets[\"train\"][:10]['input_ids']:\n",
    "    print(elem[:40])\n",
    "    print(tokenizer.decode(elem[:40]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def filtering(example):\n",
    "    flags = []\n",
    "\n",
    "    for id_list in example[\"input_ids\"]:\n",
    "\n",
    "        if id_list[-1] != 0: # last token is not a padding (the doc was probably truncated)\n",
    "            flags.append(False)\n",
    "\n",
    "        elif id_list[10] == 0: # the token in the first 10 is a padding [PAD] (the doc is <= 10 tokens, including [sep])\n",
    "            flags.append(False)\n",
    "\n",
    "        else:\n",
    "            flags.append(True)\n",
    "\n",
    "    return flags\n",
    "\n",
    "\n",
    "filtered_datasets = tokenized_datasets.filter(filtering,\n",
    "                                         batched = True,\n",
    "                                         num_proc = cpu_count(),\n",
    "                                        )\n",
    "\n",
    "filtered_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# save tokenized dataset locally:\n",
    "filtered_datasets.save_to_disk(filtered_datasets_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "filtered_datasets = load_from_disk(filtered_datasets_name)\n",
    "\n",
    "filtered_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make training dataset smaller for easier experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "    num_rows: 56726693\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset = tokenized_datasets[\"train\"] #.select([i for i in range( int(1 * len(tokenized_datasets[\"train\"])) )])\n",
    "\n",
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "    num_rows: 6302966\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_dataset = tokenized_datasets[\"test\"] #.select([i for i in range( int(1 * len(tokenized_datasets[\"test\"])) )])\n",
    "\n",
    "evaluation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert to pytorch format and remove token type id, which is ignored by modernbert"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "training_dataset.set_format(type=\"pt\", columns=['input_ids', 'attention_mask', 'special_tokens_mask'])\n",
    "\n",
    "evaluation_dataset.set_format(type=\"pt\", columns=['input_ids', 'attention_mask', 'special_tokens_mask'])\n",
    "\n",
    "training_dataset, evaluation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import a pre-configured bert and choose a name for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"Modern/{4.6}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModernBertConfig {\n",
       "  \"_name_or_path\": \"ModernBERT-base\",\n",
       "  \"architectures\": [\n",
       "    \"ModernBertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 50281,\n",
       "  \"classifier_activation\": \"gelu\",\n",
       "  \"classifier_bias\": false,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"classifier_pooling\": \"mean\",\n",
       "  \"cls_token_id\": 50281,\n",
       "  \"decoder_bias\": true,\n",
       "  \"deterministic_flash_attn\": false,\n",
       "  \"embedding_dropout\": 0.0,\n",
       "  \"eos_token_id\": 50282,\n",
       "  \"global_attn_every_n_layers\": 3,\n",
       "  \"global_rope_theta\": 160000.0,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_activation\": \"gelu\",\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_cutoff_factor\": 2.0,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 1152,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"local_attention\": 128,\n",
       "  \"local_rope_theta\": 10000.0,\n",
       "  \"max_position_embeddings\": 8192,\n",
       "  \"mlp_bias\": false,\n",
       "  \"mlp_dropout\": 0.0,\n",
       "  \"model_type\": \"modernbert\",\n",
       "  \"norm_bias\": false,\n",
       "  \"norm_eps\": 1e-05,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 22,\n",
       "  \"pad_token_id\": 50283,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"reference_compile\": false,\n",
       "  \"repad_logits_with_grad\": false,\n",
       "  \"sep_token_id\": 50282,\n",
       "  \"sparse_pred_ignore_index\": -100,\n",
       "  \"sparse_prediction\": false,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.48.1\",\n",
       "  \"vocab_size\": 50368\n",
       "}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ModernBertConfig\n",
    "\n",
    "config = ModernBertConfig.from_pretrained(\"answerdotai/ModernBERT-base\", reference_compile=False)\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModernBertConfig {\n",
       "  \"_name_or_path\": \"ModernBERT-base\",\n",
       "  \"architectures\": [\n",
       "    \"ModernBertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 2,\n",
       "  \"classifier_activation\": \"gelu\",\n",
       "  \"classifier_bias\": false,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"classifier_pooling\": \"mean\",\n",
       "  \"cls_token_id\": 2,\n",
       "  \"decoder_bias\": true,\n",
       "  \"deterministic_flash_attn\": false,\n",
       "  \"embedding_dropout\": 0.0,\n",
       "  \"eos_token_id\": 3,\n",
       "  \"global_attn_every_n_layers\": 3,\n",
       "  \"global_rope_theta\": 160000.0,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_activation\": \"gelu\",\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_cutoff_factor\": 2.0,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 1152,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"local_attention\": 128,\n",
       "  \"local_rope_theta\": 10000.0,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"mlp_bias\": false,\n",
       "  \"mlp_dropout\": 0.0,\n",
       "  \"model_type\": \"modernbert\",\n",
       "  \"norm_bias\": false,\n",
       "  \"norm_eps\": 1e-05,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 22,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"reference_compile\": false,\n",
       "  \"repad_logits_with_grad\": false,\n",
       "  \"sep_token_id\": 3,\n",
       "  \"sparse_pred_ignore_index\": -100,\n",
       "  \"sparse_prediction\": false,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.48.1\",\n",
       "  \"vocab_size\": 32768\n",
       "}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# diminish the specs\n",
    "\n",
    "# {bert-small} with max_position_embedding = 128\n",
    "\n",
    "config.vocab_size = vocabulary_size\n",
    "config.max_position_embeddings = 512\n",
    "config.local_attention = 128\n",
    "config.pad_token_id = 0\n",
    "config.bos_token_id = 2\n",
    "config.cls_token_id = 2\n",
    "config.eos_token_id = 3\n",
    "config.sep_token_id = 3\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train for MLM task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters:  136120832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModernBertForMaskedLM(\n",
       "  (model): ModernBertModel(\n",
       "    (embeddings): ModernBertEmbeddings(\n",
       "      (tok_embeddings): Embedding(32768, 768, padding_idx=0)\n",
       "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): ModernBertEncoderLayer(\n",
       "        (attn_norm): Identity()\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1-2): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (4-5): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (6): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (7-8): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (9): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (10-11): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (12): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (13-14): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (15): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (16-17): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (18): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (19-20): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (21): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (head): ModernBertPredictionHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=False)\n",
       "    (act): GELUActivation()\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Linear(in_features=768, out_features=32768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ModernBertForMaskedLM\n",
    "\n",
    "model = ModernBertForMaskedLM(config=config)\n",
    "\n",
    "print(\"parameters: \", model.num_parameters())\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model architecture and configurations to facilitate loading afterwards\n",
    "\n",
    "model.save_pretrained(f\"models/{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to mask the input randomly, we will use a data collator, which will randomly tokenize 30% of the input as [MASK] \n",
    "\n",
    "it is a dynamical masking (such as the one used in roBERTa), meaning that each time it a document is given as the input in the model, as random porcentile will the masked. \n",
    "\n",
    "in static masking, all the dataset is prior masked, meaning that the masked tokens is predefinied and if you run multiple epochs, it won't make a difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# mask 30% of the tokens\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm = True,\n",
    "    mlm_probability=0.3\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the training arguments, \n",
    "\n",
    "- we will train the model passing through the dataset twice (epoch == 2)\n",
    "\n",
    "- the batch size (number of inputs trained per iteration) is 128\n",
    "\n",
    "- RoBERTa proved that bigger batches will output better results, that why we also use gradient accumulation steps \n",
    "\n",
    "- only after 4 {gradient_accumulation_steps}, the model will backpropagate and update parameters.\n",
    "\n",
    "- each 1_000 iterations (steps) the model parameters will be locally saved\n",
    "\n",
    "- the number of saves or checkpoints saved simultaneusly is 3\n",
    "\n",
    "- it will used mixed precision of fp16, meaning that sometimes (when the precision won't affect the accuracy),\n",
    "it will not use the float of 32 bits and instead use float of 16 bits, speeding up the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluate\n",
    "\n",
    "# classification_metrics = evaluate.combine([\"accuracy\"])\n",
    "\n",
    "# def compute_metrics_modern(eval_pred):\n",
    "\n",
    "#     labels = eval_pred.label_ids\n",
    "#     preds = eval_pred.predictions.argmax(-1)\n",
    "\n",
    "#     return classification_metrics.compute(predictions=preds.flatten(), references=labels.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import get_wsd_schedule\n",
    "from torch.optim import AdamW\n",
    "\n",
    "total_steps = 500_000\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'training/{model_name}',\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "    # num_train_epochs=1,                     # number of training epochs\n",
    "    max_steps=total_steps,\n",
    "    # max_steps=100,\n",
    "\n",
    "    gradient_accumulation_steps = 1,\n",
    "    # eval_accumulation_steps = 1,\n",
    "\n",
    "    per_device_train_batch_size=32,          # batch size for training\n",
    "    # per_device_eval_batch_size=32,           # batch size for evaluation\n",
    "\n",
    "    \n",
    "    logging_strategy=\"steps\",\n",
    "    logging_first_step=True, # output the initial loss\n",
    "    logging_steps=1_000,\n",
    "    logging_dir=f\"training-logs/{model_name}\",\n",
    "    report_to=[\"tensorboard\"],\n",
    "\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1_000,                      # Save checkpoints every 100 steps\n",
    "    save_total_limit=5,                  # Limit the total number of saved checkpoints\n",
    "\n",
    "    fp16=True,                            # Enable mixed precision for faster training\n",
    "\n",
    "    # learning_rate=8e-4,\n",
    "    # weight_decay=1e-2,\n",
    "    # adam_beta1=0.9,\n",
    "    # adam_beta2=0.999,\n",
    "    # adam_epsilon=1e-06,\n",
    "    # lr_scheduler_type=\n",
    ")\n",
    "\n",
    "# # Create default optimizer\n",
    "# optimizer = AdamW(\n",
    "#     model.parameters(),\n",
    "#     lr = 8e-4,\n",
    "#     weight_decay=1e-2,\n",
    "#     betas = (0.9, 0.999),\n",
    "# )\n",
    "\n",
    "# scheduler = get_wsd_schedule(\n",
    "#     AdamW,                  # Your optimizer\n",
    "#     num_warmup_steps=total_steps * 0.1,   # Number of warmup steps\n",
    "#     num_decay_steps=total_steps * 0.1,   # Number of decay steps\n",
    "#     # num_training_steps=total_steps,  # Total number of training steps\n",
    "#     num_stable_steps=total_steps * 0.8,   # Number of stable steps\n",
    "#     #warmup_type=\"linear\",   # Warmup type\n",
    "#     #decay_type=\"1-sqrt\",    # Decay type\n",
    "#     min_lr_ratio=0.0,       # Minimum learning rate ratio\n",
    "#     # num_cycles=0.5,         # Number of cosine cycles\n",
    "# )\n",
    "\n",
    "    # lr_scheduler_type=\"warmup_stable_decay\",\n",
    "    # lr_scheduler_kwargs = {\n",
    "    #     #\"optimizer\": \"AdamW\",\n",
    "    #     #\"num_warmup_steps\": 25,l\n",
    "    #     \"num_decay_steps\": 25,\n",
    "    #     #\"num_training_steps\": 1000,\n",
    "    #     \"num_stable_steps\": 950,\n",
    "    #     # \"warmup_type\": \"linear\",\n",
    "    #     #\"decay_type\": \"1-sqrt\",\n",
    "    #     #\"min_lr_ratio\": 0,\n",
    "    # },\n",
    "    # # warmup_steps=25,\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                        # Model to train\n",
    "    args=training_args,                 # Training arguments\n",
    "    train_dataset=training_dataset,     # Training dataset\n",
    "    # eval_dataset=evaluation_dataset,    # Evaluation dataset\n",
    "    data_collator=data_collator,\n",
    "    # optimizers=(optimizer, scheduler),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['decoder.weight'].\n",
      "/work1/lgarcia/wallacelw/ModernBERT/myenv/lib64/python3.9/site-packages/transformers/trainer.py:3441: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "/work1/lgarcia/wallacelw/ModernBERT/myenv/lib64/python3.9/site-packages/transformers/trainer.py:3105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n",
      "/share/sw/ai/pytorch/2.5.1/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500002' max='500000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500000/500000 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True)\n",
    "\n",
    "trainer.save_model(f\"trained/{model_name}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "trainer.save_model(f\"trained/{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the self trained model and create a pipeline that will:\n",
    "\n",
    "- tokenize the input\n",
    " \n",
    "- push to the model\n",
    "\n",
    "- output the top 5 token that the model predict to fill each masked token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModernBertForMaskedLM(\n",
       "  (model): ModernBertModel(\n",
       "    (embeddings): ModernBertEmbeddings(\n",
       "      (tok_embeddings): Embedding(32768, 768, padding_idx=0)\n",
       "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): ModernBertEncoderLayer(\n",
       "        (attn_norm): Identity()\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1-2): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (4-5): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (6): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (7-8): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (9): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (10-11): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (12): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (13-14): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (15): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (16-17): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (18): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (19-20): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (21): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (head): ModernBertPredictionHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=False)\n",
       "    (act): GELUActivation()\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Linear(in_features=768, out_features=32768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, ModernBertForMaskedLM\n",
    "\n",
    "model = ModernBertForMaskedLM.from_pretrained(f\"trained/{model_name}\")\n",
    "\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/sw/ai/pytorch/2.5.1/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='24621' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   45/24621 00:28 < 4:24:20, 1.55 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 36\u001b[0m\n\u001b[1;32m     26\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     27\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,   \n\u001b[1;32m     28\u001b[0m     args\u001b[38;5;241m=\u001b[39mevaluation_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Print the results\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(eval_results)\n",
      "File \u001b[0;32m/work1/lgarcia/wallacelw/ModernBERT/myenv/lib64/python3.9/site-packages/transformers/trainer.py:4073\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4070\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   4072\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 4073\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4074\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   4077\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   4078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4081\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4083\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   4084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/work1/lgarcia/wallacelw/ModernBERT/myenv/lib64/python3.9/site-packages/transformers/trainer.py:4257\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4254\u001b[0m observed_num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   4256\u001b[0m \u001b[38;5;66;03m# Main evaluation loop\u001b[39;00m\n\u001b[0;32m-> 4257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m   4258\u001b[0m     \u001b[38;5;66;03m# Update the observed num examples\u001b[39;00m\n\u001b[1;32m   4259\u001b[0m     observed_batch_size \u001b[38;5;241m=\u001b[39m find_batch_size(inputs)\n\u001b[1;32m   4260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m observed_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/work1/lgarcia/wallacelw/ModernBERT/myenv/lib64/python3.9/site-packages/accelerate/data_loader.py:561\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;66;03m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 561\u001b[0m         current_batch \u001b[38;5;241m=\u001b[39m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_non_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state_dict()\n\u001b[1;32m    563\u001b[0m     next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n",
      "File \u001b[0;32m/work1/lgarcia/wallacelw/ModernBERT/myenv/lib64/python3.9/site-packages/accelerate/utils/operations.py:156\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    154\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu:0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/work1/lgarcia/wallacelw/ModernBERT/myenv/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:820\u001b[0m, in \u001b[0;36mBatchEncoding.to\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 820\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    821\u001b[0m         k: v\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39mnon_blocking) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    823\u001b[0m     }\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    825\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/work1/lgarcia/wallacelw/ModernBERT/myenv/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:821\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 821\u001b[0m         k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    823\u001b[0m     }\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    825\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "\n",
    "evaluation_args = TrainingArguments(\n",
    "    output_dir=f'evaluating/{model_name}',\n",
    "\n",
    "    do_train=False,\n",
    "    eval_strategy = \"steps\", \n",
    "    eval_steps=1_000,\n",
    "    # eval_accumulation_steps=1,\n",
    "\n",
    "    per_device_eval_batch_size=32,           # batch size for evaluation\n",
    "\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1_000,\n",
    "    logging_dir=f'evaluation-logs/{model_name}',                # directory for storing logs\n",
    "    report_to=[\"tensorboard\"],\n",
    "\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1_000,                      # Save checkpoints every 100 steps\n",
    "    save_total_limit=5,                  # Limit the total number of saved checkpoints\n",
    "\n",
    "    fp16=True,    \n",
    ")\n",
    "\n",
    "evaluator = Trainer(\n",
    "    model=model,   \n",
    "    args=evaluation_args,\n",
    "    train_dataset=[],\n",
    "    eval_dataset=evaluation_dataset,         # Evaluation dataset\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = evaluator.evaluate()\n",
    "\n",
    "# Print the results\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 5, 11, 18, 32, 7471, 4, 5, 6, 36, 8060, 5, 12, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "test1 = \"Eu não entendi [MASK], como proceder.\"\n",
    "\n",
    "out = tokenizer(test1)\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '▁',\n",
       " 'e',\n",
       " 'u',\n",
       " '▁não',\n",
       " '▁entendi',\n",
       " '[MASK]',\n",
       " '▁',\n",
       " ',',\n",
       " '▁como',\n",
       " '▁proceder',\n",
       " '▁',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(out['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.1924876570701599,\n",
       "  'token': 268,\n",
       "  'token_str': 'nada',\n",
       "  'sequence': 'eu não entendi nada , como proceder .'},\n",
       " {'score': 0.1782321184873581,\n",
       "  'token': 221,\n",
       "  'token_str': 'direito',\n",
       "  'sequence': 'eu não entendi direito , como proceder .'},\n",
       " {'score': 0.1367182582616806,\n",
       "  'token': 87,\n",
       "  'token_str': 'bem',\n",
       "  'sequence': 'eu não entendi bem , como proceder .'},\n",
       " {'score': 0.05346071347594261,\n",
       "  'token': 164,\n",
       "  'token_str': 'tudo',\n",
       "  'sequence': 'eu não entendi tudo , como proceder .'},\n",
       " {'score': 0.04698919877409935,\n",
       "  'token': 73,\n",
       "  'token_str': 'muito',\n",
       "  'sequence': 'eu não entendi muito , como proceder .'}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.1659652292728424,\n",
       "  'token': 2471,\n",
       "  'token_str': 'chão',\n",
       "  'sequence': 'atiraram o pau no chão'},\n",
       " {'score': 0.06266004592180252,\n",
       "  'token': 1599,\n",
       "  'token_str': 'marido',\n",
       "  'sequence': 'atiraram o pau no marido'},\n",
       " {'score': 0.0401596799492836,\n",
       "  'token': 4321,\n",
       "  'token_str': 'rapaz',\n",
       "  'sequence': 'atiraram o pau no rapaz'},\n",
       " {'score': 0.027083080261945724,\n",
       "  'token': 638,\n",
       "  'token_str': 'cara',\n",
       "  'sequence': 'atiraram o pau no cara'},\n",
       " {'score': 0.025046631693840027,\n",
       "  'token': 11493,\n",
       "  'token_str': 'rabo',\n",
       "  'sequence': 'atiraram o pau no rabo'}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"Atiraram o pau no [MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'score': 0.25275641679763794,\n",
       "   'token': 134,\n",
       "   'token_str': 'dois',\n",
       "   'sequence': '[CLS] essa frase tem dois tokens , portanto ele vai gerar dois[MASK][SEP]'},\n",
       "  {'score': 0.13312558829784393,\n",
       "   'token': 194,\n",
       "   'token_str': 'três',\n",
       "   'sequence': '[CLS] essa frase tem três tokens , portanto ele vai gerar dois[MASK][SEP]'},\n",
       "  {'score': 0.09293114393949509,\n",
       "   'token': 351,\n",
       "   'token_str': 'quatro',\n",
       "   'sequence': '[CLS] essa frase tem quatro tokens , portanto ele vai gerar dois[MASK][SEP]'},\n",
       "  {'score': 0.07274574786424637,\n",
       "   'token': 470,\n",
       "   'token_str': 'cinco',\n",
       "   'sequence': '[CLS] essa frase tem cinco tokens , portanto ele vai gerar dois[MASK][SEP]'},\n",
       "  {'score': 0.0429421029984951,\n",
       "   'token': 750,\n",
       "   'token_str': 'seis',\n",
       "   'sequence': '[CLS] essa frase tem seis tokens , portanto ele vai gerar dois[MASK][SEP]'}],\n",
       " [{'score': 0.040673743933439255,\n",
       "   'token': 1086,\n",
       "   'token_str': 'efeitos',\n",
       "   'sequence': '[CLS] essa frase tem[MASK] tokens , portanto ele vai gerar dois efeitos[SEP]'},\n",
       "  {'score': 0.04043954983353615,\n",
       "   'token': 2114,\n",
       "   'token_str': 'números',\n",
       "   'sequence': '[CLS] essa frase tem[MASK] tokens , portanto ele vai gerar dois números[SEP]'},\n",
       "  {'score': 0.02711174264550209,\n",
       "   'token': 662,\n",
       "   'token_str': 'resultados',\n",
       "   'sequence': '[CLS] essa frase tem[MASK] tokens , portanto ele vai gerar dois resultados[SEP]'},\n",
       "  {'score': 0.0267016738653183,\n",
       "   'token': 1029,\n",
       "   'token_str': 'eventos',\n",
       "   'sequence': '[CLS] essa frase tem[MASK] tokens , portanto ele vai gerar dois eventos[SEP]'},\n",
       "  {'score': 0.02324492670595646,\n",
       "   'token': 3135,\n",
       "   'token_str': 'times',\n",
       "   'sequence': '[CLS] essa frase tem[MASK] tokens , portanto ele vai gerar dois times[SEP]'}]]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"Essa frase tem [MASK] tokens, portanto ele vai gerar dois [MASK]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER task\n",
    "\n",
    "### Fine-tuning for for NER task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the curated dataset for portuguese NER task in the legal scope.\n",
    "\n",
    "It already has the train, validation and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 7828\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1177\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1390\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "lenerNER = load_dataset(\"peluz/lener_br\", \n",
    "                        trust_remote_code=True, \n",
    "                        num_proc=cpu_count())\n",
    "\n",
    "lenerNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['EMENTA', ':', 'APELAÇÃO', 'CÍVEL', '-', 'AÇÃO', 'DE', 'INDENIZAÇÃO', 'POR', 'DANOS', 'MORAIS', '-', 'PRELIMINAR', '-', 'ARGUIDA', 'PELO', 'MINISTÉRIO', 'PÚBLICO', 'EM', 'GRAU', 'RECURSAL', '-', 'NULIDADE', '-', 'AUSÊNCIA', 'DE', 'INTERVENÇÃO', 'DO', 'PARQUET', 'NA', 'INSTÂNCIA', 'A', 'QUO', '-', 'PRESENÇA', 'DE', 'INCAPAZ', '-', 'PREJUÍZO', 'EXISTENTE', '-', 'PRELIMINAR', 'ACOLHIDA', '-', 'NULIDADE', 'RECONHECIDA', '.']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print( lenerNER[\"train\"][0][\"id\"] ) # id of the document [0, 70)\n",
    "    \n",
    "print( lenerNER[\"train\"][0][\"tokens\"] ) # words\n",
    "\n",
    "print( lenerNER[\"train\"][0][\"ner_tags\"] ) # tags for the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each ner_tag id corresponds to one of the following labels:\n",
    "\n",
    "**BIO notation:**\n",
    "\n",
    "B - means that is the beginning of a entity\n",
    "\n",
    "I - means that it inside the entity (after a B tag)\n",
    "\n",
    "O - means that this token a non-entity tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-ORGANIZACAO',\n",
       " 'I-ORGANIZACAO',\n",
       " 'B-PESSOA',\n",
       " 'I-PESSOA',\n",
       " 'B-TEMPO',\n",
       " 'I-TEMPO',\n",
       " 'B-LOCAL',\n",
       " 'I-LOCAL',\n",
       " 'B-LEGISLACAO',\n",
       " 'I-LEGISLACAO',\n",
       " 'B-JURISPRUDENCIA',\n",
       " 'I-JURISPRUDENCIA']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = lenerNER[\"train\"].features[f\"ner_tags\"].feature.names\n",
    "\n",
    "label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word can be split into several tokens by our pre trained tokenizer, let's see an example:\n",
    "\n",
    "word \"PARQUET\" was split into 2 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EMENTA', ':', 'APELAÇÃO', 'CÍVEL', '-', 'AÇÃO', 'DE', 'INDENIZAÇÃO', 'POR', 'DANOS', 'MORAIS', '-', 'PRELIMINAR', '-', 'ARGUIDA', 'PELO', 'MINISTÉRIO', 'PÚBLICO', 'EM', 'GRAU', 'RECURSAL', '-', 'NULIDADE', '-', 'AUSÊNCIA', 'DE', 'INTERVENÇÃO', 'DO', 'PARQUET', 'NA', 'INSTÂNCIA', 'A', 'QUO', '-', 'PRESENÇA', 'DE', 'INCAPAZ', '-', 'PREJUÍZO', 'EXISTENTE', '-', 'PRELIMINAR', 'ACOLHIDA', '-', 'NULIDADE', 'RECONHECIDA', '.']\n",
      "[2, 16, 2091, 5, 50, 12806, 5, 7219, 5, 20, 5, 168, 9, 4873, 33, 1959, 3520, 5, 20, 8645, 5, 20, 6506, 18, 709, 53, 418, 206, 16, 1433, 17707, 5, 20, 10922, 5, 20, 1885, 9, 2463, 14, 1306, 41, 28, 5583, 5, 7, 5495, 8, 5, 20, 605, 9, 7982, 5, 20, 3566, 3580, 5, 20, 8645, 5, 7, 8247, 5, 20, 10922, 4134, 5, 12, 3]\n",
      "['[CLS]', '▁em', 'enta', '▁', ':', '▁apelação', '▁', 'cível', '▁', '-', '▁', 'ação', '▁de', '▁indenização', '▁por', '▁danos', '▁morais', '▁', '-', '▁preliminar', '▁', '-', '▁arg', 'u', 'ida', '▁pelo', '▁ministério', '▁público', '▁em', '▁grau', '▁recursal', '▁', '-', '▁nulidade', '▁', '-', '▁ausência', '▁de', '▁intervenção', '▁do', '▁parque', 't', '▁na', '▁instância', '▁', 'a', '▁qu', 'o', '▁', '-', '▁presença', '▁de', '▁incapaz', '▁', '-', '▁prejuízo', '▁existente', '▁', '-', '▁preliminar', '▁', 'a', 'colhida', '▁', '-', '▁nulidade', '▁reconhecida', '▁', '.', '[SEP]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "exampleNER = lenerNER[\"train\"][0]\n",
    "\n",
    "example_tokenized_input = tokenizer(exampleNER[\"tokens\"], is_split_into_words = True)\n",
    "\n",
    "example_tokens = tokenizer.convert_ids_to_tokens(example_tokenized_input[\"input_ids\"])\n",
    "\n",
    "print(exampleNER[\"tokens\"])\n",
    "print(example_tokenized_input[\"input_ids\"])\n",
    "print(example_tokens)\n",
    "print(exampleNER[\"ner_tags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**word_id:** the index of the word given prior to the tokenization!\n",
    "\n",
    "check the example with word index 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None [CLS]\n",
      "0 ▁em\n",
      "0 enta\n",
      "1 ▁\n",
      "1 :\n",
      "2 ▁apelação\n",
      "3 ▁\n",
      "3 cível\n",
      "4 ▁\n",
      "4 -\n",
      "5 ▁\n",
      "5 ação\n",
      "6 ▁de\n",
      "7 ▁indenização\n",
      "8 ▁por\n",
      "9 ▁danos\n",
      "10 ▁morais\n",
      "11 ▁\n",
      "11 -\n",
      "12 ▁preliminar\n",
      "13 ▁\n",
      "13 -\n",
      "14 ▁arg\n",
      "14 u\n",
      "14 ida\n",
      "15 ▁pelo\n",
      "16 ▁ministério\n",
      "17 ▁público\n",
      "18 ▁em\n",
      "19 ▁grau\n",
      "20 ▁recursal\n",
      "21 ▁\n",
      "21 -\n",
      "22 ▁nulidade\n",
      "23 ▁\n",
      "23 -\n",
      "24 ▁ausência\n",
      "25 ▁de\n",
      "26 ▁intervenção\n",
      "27 ▁do\n",
      "28 ▁parque\n",
      "28 t\n",
      "29 ▁na\n",
      "30 ▁instância\n",
      "31 ▁\n",
      "31 a\n",
      "32 ▁qu\n",
      "32 o\n",
      "33 ▁\n",
      "33 -\n",
      "34 ▁presença\n",
      "35 ▁de\n",
      "36 ▁incapaz\n",
      "37 ▁\n",
      "37 -\n",
      "38 ▁prejuízo\n",
      "39 ▁existente\n",
      "40 ▁\n",
      "40 -\n",
      "41 ▁preliminar\n",
      "42 ▁\n",
      "42 a\n",
      "42 colhida\n",
      "43 ▁\n",
      "43 -\n",
      "44 ▁nulidade\n",
      "45 ▁reconhecida\n",
      "46 ▁\n",
      "46 .\n",
      "None [SEP]\n"
     ]
    }
   ],
   "source": [
    "# if a word is split into 2 tokens, both will share the same word_id\n",
    "\n",
    "example_word_ids = example_tokenized_input.word_ids()\n",
    "\n",
    "for i, j in zip(example_word_ids, example_tokens):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the input and preprocess. \n",
    "\n",
    "We need also to align, because some words were split into several tokens\n",
    "\n",
    "label of [CLS], [SEP] and for tokens that are suffixes (##) have label == -100 (meaning that it will be ignored for the ner tag prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59019a28ac0e450190dda5d9be02cddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=128):   0%|          | 0/7828 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51ab7b422da4e51b0dd185cb60f3033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=128):   0%|          | 0/1177 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "185bbd0938c9466ba9bbf7f1e7e3b02b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=128):   0%|          | 0/1390 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    \n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], \n",
    "                                 max_length=context_size,\n",
    "                                 truncation=True, \n",
    "                                 is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "\n",
    "            if word_idx is None: # [CLS], [SEP]\n",
    "                label_ids.append(-100)\n",
    "\n",
    "            elif word_idx != previous_word_idx: # new word\n",
    "                label_ids.append(label[word_idx])\n",
    "\n",
    "            else: # suffix of a previous token\n",
    "                label_ids.append(-100)\n",
    "\n",
    "            previous_word_idx = word_idx\n",
    "            \n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_lenerNER = lenerNER.map(tokenize_and_align_labels,\n",
    "                                  batched=True,\n",
    "                                  num_proc=cpu_count(),\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 7828\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1177\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1390\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_lenerNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "ner_collator = DataCollatorForTokenClassification(tokenizer=tokenizer,\n",
    "                                                  padding=True,\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load a evaluation metric that will be used to compute the total evaluation for a sequence (seq eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the method to compute metrics (precision, recall, f1, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics_ner(p):\n",
    "\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dictionaries to convert id to label and vice-versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-ORGANIZACAO', 2: 'I-ORGANIZACAO', 3: 'B-PESSOA', 4: 'I-PESSOA', 5: 'B-TEMPO', 6: 'I-TEMPO', 7: 'B-LOCAL', 8: 'I-LOCAL', 9: 'B-LEGISLACAO', 10: 'I-LEGISLACAO', 11: 'B-JURISPRUDENCIA', 12: 'I-JURISPRUDENCIA'}\n",
      "{'O': 0, 'B-ORGANIZACAO': 1, 'I-ORGANIZACAO': 2, 'B-PESSOA': 3, 'I-PESSOA': 4, 'B-TEMPO': 5, 'I-TEMPO': 6, 'B-LOCAL': 7, 'I-LOCAL': 8, 'B-LEGISLACAO': 9, 'I-LEGISLACAO': 10, 'B-JURISPRUDENCIA': 11, 'I-JURISPRUDENCIA': 12}\n"
     ]
    }
   ],
   "source": [
    "id2label = {}\n",
    "label2id = {}\n",
    "\n",
    "for i, lab in enumerate(label_list):\n",
    "\n",
    "    id2label[i] = lab\n",
    "    label2id[lab] = i\n",
    "\n",
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the pre trained model of bert to fine tune it for NER task, attaching a Token Classification HEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForTokenClassification were not initialized from the model checkpoint at trained/Modern/4.6 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModernBertForTokenClassification(\n",
       "  (model): ModernBertModel(\n",
       "    (embeddings): ModernBertEmbeddings(\n",
       "      (tok_embeddings): Embedding(32768, 768, padding_idx=0)\n",
       "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): ModernBertEncoderLayer(\n",
       "        (attn_norm): Identity()\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1-2): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (4-5): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (6): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (7-8): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (9): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (10-11): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (12): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (13-14): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (15): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (16-17): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (18): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (19-20): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (21): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (head): ModernBertPredictionHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=False)\n",
       "    (act): GELUActivation()\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (drop): Dropout(p=0.0, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=13, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ModernBertForTokenClassification\n",
    "\n",
    "ner_model = ModernBertForTokenClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=f\"trained/{model_name}\",\n",
    "    num_labels = len(label_list),\n",
    "    id2label = id2label,\n",
    "    label2id= label2id,\n",
    ")\n",
    "\n",
    "ner_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train it in batches of size 16, while also computing the evaluation for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "ner_training_args = TrainingArguments(\n",
    "    output_dir = f\"trained/NER/{model_name}\",\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "    num_train_epochs = 2,\n",
    "\n",
    "    eval_strategy = \"epoch\",\n",
    "\n",
    "    gradient_accumulation_steps = 1,\n",
    "    eval_accumulation_steps = 1,\n",
    "\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    \n",
    "    learning_rate = 2e-5,\n",
    "    weight_decay = 0.01,\n",
    "    \n",
    "    logging_first_step=True,\n",
    "    save_strategy = \"steps\",\n",
    "    save_steps=100,                      # Save checkpoints every 100 steps\n",
    "    save_total_limit=5,\n",
    "\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "ner_trainer = Trainer(\n",
    "    model = ner_model,\n",
    "    train_dataset = tokenized_lenerNER[\"train\"],\n",
    "    eval_dataset = tokenized_lenerNER[\"validation\"],\n",
    "    data_collator = ner_collator,\n",
    "    compute_metrics = compute_metrics_ner,\n",
    "    args = ner_training_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/sw/ai/pytorch/2.5.1/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='124' max='124' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [124/124 01:07, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.004500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.709213</td>\n",
       "      <td>0.812981</td>\n",
       "      <td>0.757560</td>\n",
       "      <td>0.975295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.004500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.756851</td>\n",
       "      <td>0.835534</td>\n",
       "      <td>0.794248</td>\n",
       "      <td>0.977822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/sw/ai/pytorch/2.5.1/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/share/sw/ai/pytorch/2.5.1/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=124, training_loss=0.1392117084995393, metrics={'train_runtime': 68.8142, 'train_samples_per_second': 227.511, 'train_steps_per_second': 1.802, 'total_flos': 3590367998315592.0, 'train_loss': 0.1392117084995393, 'epoch': 2.0})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test NER Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_ner_name = f\"trained/NER/{model_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def tag_sentence(text:str):\n",
    "\n",
    "    # convert our text to a  tokenized sequence\n",
    "    inputs = tokenizer(text, truncation=True, return_tensors=\"pt\", return_token_type_ids=False).to(\"cuda:0\")\n",
    "\n",
    "    # get outputs\n",
    "    outputs = ner_model(**inputs)\n",
    "\n",
    "    # convert to probabilities with softmax\n",
    "    probs = outputs[0][0].softmax(1)\n",
    "\n",
    "    # get the tags with the highest probability\n",
    "    word_tags = [(tokenizer.decode(inputs['input_ids'][0][i].item()), id2label[tagid.item()]) \n",
    "                  for i, tagid in enumerate (probs.argmax(axis=1))]\n",
    "\n",
    "    return pd.DataFrame(word_tags, columns=['word', 'tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>o</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>senador</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>de</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>são</td>\n",
       "      <td>B-LOCAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>paulo</td>\n",
       "      <td>I-LOCAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>decidiu</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>falta</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>r</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>a</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>apelação</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>movida</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pelo</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>supremo</td>\n",
       "      <td>B-ORGANIZACAO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tribunal</td>\n",
       "      <td>I-ORGANIZACAO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>federal</td>\n",
       "      <td>I-ORGANIZACAO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[SEP]</td>\n",
       "      <td>B-LOCAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word            tag\n",
       "0      [CLS]              O\n",
       "1                         O\n",
       "2          o              O\n",
       "3    senador              O\n",
       "4         de              O\n",
       "5        são        B-LOCAL\n",
       "6      paulo        I-LOCAL\n",
       "7    decidiu              O\n",
       "8      falta              O\n",
       "9          r              O\n",
       "10                        O\n",
       "11         a              O\n",
       "12  apelação              O\n",
       "13                        O\n",
       "14         ,              O\n",
       "15                        O\n",
       "16    movida              O\n",
       "17      pelo              O\n",
       "18   supremo  B-ORGANIZACAO\n",
       "19  tribunal  I-ORGANIZACAO\n",
       "20   federal  I-ORGANIZACAO\n",
       "21                        O\n",
       "22         .              O\n",
       "23     [SEP]        B-LOCAL"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"O senador de São Paulo decidiu faltar a apelação, movida pelo Supremo Tribunal Federal.\"\n",
    "\n",
    "tag_sentence(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, evaluate the fine tuned model in the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/sw/ai/pytorch/2.5.1/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/11 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.044777218252420425,\n",
       " 'eval_precision': 0.8151057401812689,\n",
       " 'eval_recall': 0.8910171730515192,\n",
       " 'eval_f1': 0.8513726727674346,\n",
       " 'eval_accuracy': 0.9872004068744834,\n",
       " 'eval_runtime': 4.507,\n",
       " 'eval_samples_per_second': 308.407,\n",
       " 'eval_steps_per_second': 2.441,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_trainer.evaluate(eval_dataset=tokenized_lenerNER[\"test\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
