{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8267ea0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.48.1\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from multiprocessing import cpu_count\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "# set cache directory out of $HOME to $WORK\n",
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"cache/\"\n",
    "default_cache_dir = \"cache/\"\n",
    "\n",
    "import transformers \n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "131c7f1c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "wikipedia = load_dataset(\"wikimedia/wikipedia\", \n",
    "                         \"20231101.pt\", \n",
    "                         split=\"train\", \n",
    "                         num_proc=cpu_count(),\n",
    "                         cache_dir=default_cache_dir,\n",
    "                         trust_remote_code=True,\n",
    "                         )\n",
    "\n",
    "wikipedia = wikipedia.remove_columns([col for col in wikipedia.column_names if col != \"text\"])\n",
    "\n",
    "wikipedia"
   ]
  },
  {
   "cell_type": "raw",
   "id": "148f77ef",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "brwac = load_dataset(\"brwac\", \n",
    "                    data_dir=\"dataset-brwac\",\n",
    "                    split=\"train\", \n",
    "                    num_proc=cpu_count(),\n",
    "                    cache_dir=default_cache_dir,\n",
    "                    trust_remote_code=True,\n",
    "                    )\n",
    "\n",
    "brwac = brwac.remove_columns([col for col in brwac.column_names if col != \"text\"])\n",
    "\n",
    "brwac"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac8b433c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from ftfy import fix_text\n",
    "\n",
    "for para in brwac[0][\"text\"][\"paragraphs\"]:\n",
    "    for doc in para:\n",
    "        soup = BeautifulSoup(doc, 'html.parser')\n",
    "        print( fix_text(soup.get_text())  )\n",
    "\n",
    "def para2doc(example):\n",
    "    list_doc = []\n",
    "\n",
    "    for row in example[\"text\"]:\n",
    "        \n",
    "        str_doc = \"\"\n",
    "        \n",
    "        for para in row[\"paragraphs\"]:\n",
    "\n",
    "            for doc in para:\n",
    "                \n",
    "                soup = BeautifulSoup(doc, 'html.parser') \n",
    "\n",
    "                str_doc += fix_text(soup.get_text()) + '\\n'\n",
    "\n",
    "        list_doc.append(str_doc)\n",
    "\n",
    "    return {\"text\" : list_doc}\n",
    "\n",
    "preprocessed_brwac = brwac.map(para2doc,\n",
    "                                batched = True,\n",
    "                                remove_columns=[\"text\"],\n",
    "                                num_proc = cpu_count(),\n",
    "                                )\n",
    "\n",
    "preprocessed_brwac"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbf7870a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# raw_datasets = concatenate_datasets([wikipedia, brwac])\n",
    "raw_datasets = concatenate_datasets([wikipedia, preprocessed_brwac])\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b89ff05",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1294fbd8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def analize_texts(row):\n",
    "\n",
    "    list_para = []\n",
    "    list_words = []\n",
    "    list_stopwords = []\n",
    "    list_average = []\n",
    "\n",
    "    for doc in row[\"text\"]:\n",
    "        for paragraph in doc.split('\\n'):\n",
    "\n",
    "            # strip whitespaces\n",
    "            paragraph = paragraph.strip()\n",
    "\n",
    "            # skip single or empty worded paragraphs\n",
    "            if (len(paragraph.split()) < 2):\n",
    "                continue\n",
    "\n",
    "            # count how many stopwords are in the paragraph\n",
    "            stopwords_cnt = 0    \n",
    "            for word in paragraph.split():\n",
    "                for stop in stopwords:\n",
    "                    if stop.casefold() == word.casefold():  # insensitive case\n",
    "                        stopwords_cnt += 1\n",
    "                        break # count once and speed up everything\n",
    "            \n",
    "            # count non whitespace characters\n",
    "            characters = 0\n",
    "            for word in paragraph.split():\n",
    "                characters += len(word)\n",
    "\n",
    "            list_para.append(paragraph)\n",
    "            list_words.append(len(paragraph.split()))\n",
    "            list_stopwords.append(stopwords_cnt)\n",
    "            list_average.append(characters/len(paragraph.split()))\n",
    "\n",
    "    return {\"paragraphs\" : list_para, \"num_words\" : list_words, \"stopwords\" : list_stopwords, \"average\" : list_average}\n",
    "\n",
    "preprocessed_datasets = raw_datasets.map(analize_texts,\n",
    "                                         batched = True,\n",
    "                                         remove_columns=[\"text\"],\n",
    "                                         num_proc = cpu_count(),\n",
    "                                        )\n",
    "\n",
    "preprocessed_datasets = preprocessed_datasets.rename_column(\"paragraphs\", \"text\")\n",
    "\n",
    "preprocessed_datasets"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4094cfb7",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "preprocessed_datasets2 = preprocessed_datasets.filter(\n",
    "    lambda example: \n",
    "            example[\"num_words\"] >= 20 \n",
    "        and example[\"num_words\"] <= 512\n",
    "        and example[\"stopwords\"] >= 1\n",
    "        and example[\"average\"] >= 2 \n",
    "        and example[\"average\"] <= 15\n",
    "    )\n",
    "\n",
    "preprocessed_datasets2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "918287ad",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "split_dataset2 = preprocessed_datasets2.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "\n",
    "split_dataset2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fa907cd3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "split_dataset2[\"train\"][:5][\"text\"]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59fcd1c3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "preprocessed_datasets3 = preprocessed_datasets.filter(\n",
    "    lambda example: \n",
    "            example[\"num_words\"] >= 10\n",
    "        and example[\"num_words\"] < 20\n",
    "        and example[\"stopwords\"] >= 2\n",
    "        and example[\"average\"] >= 3\n",
    "        and example[\"average\"] <= 10\n",
    "    )\n",
    "\n",
    "preprocessed_datasets3"
   ]
  },
  {
   "cell_type": "raw",
   "id": "66526c28",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "split_dataset3 = preprocessed_datasets3.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "\n",
    "split_dataset3"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f3e96286",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "split_dataset3[\"train\"][:5][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a26759",
   "metadata": {},
   "source": [
    "merge"
   ]
  },
  {
   "cell_type": "raw",
   "id": "26386a0f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from datasets import concatenate_datasets, DatasetDict\n",
    "\n",
    "split_dataset = DatasetDict({\n",
    "    \"train\": concatenate_datasets([split_dataset2['train'], split_dataset3['train']]),\n",
    "    \"test\": concatenate_datasets([split_dataset2['test'], split_dataset3['test']]),\n",
    "})\n",
    "\n",
    "split_dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d95e8e74",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "split_dataset.save_to_disk(\"dataset/split_dataset\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "17811881",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "split_dataset = load_from_disk(\"dataset/split_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0b3b714",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1efdd2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"wallacelw/ModBERTBr\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "26bc84f0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def group_texts(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"text\"], \n",
    "        max_length=context_size,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = split_dataset.map(group_texts, \n",
    "                                      batched=True,\n",
    "                                      remove_columns=[\"text\", 'num_words', 'stopwords', 'average'], \n",
    "                                      num_proc=cpu_count()\n",
    "                                      )\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42f039a3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "tokenized_datasets.save_to_disk(\"dataset/tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ce8f065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f7cd313f07c40afa8d15aa8e2fbfa1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a4624d79c245e4ae98edef50a8ae40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "folder_path = \"dataset/tokenized\"\n",
    "folder_exists = os.path.isdir(folder_path)\n",
    "\n",
    "if folder_exists:\n",
    "    tokenized_datasets = load_from_disk(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e146c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 98904411\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 10989380\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10d1595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_model_name = f\"trained/Modern/{4.6}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61409047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ModernBertForMaskedLM\n",
    "\n",
    "model = ModernBertForMaskedLM.from_pretrained(old_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e3b4074",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"Modern/{5.0}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecbb170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# mask 30% of the tokens\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm = True,\n",
    "    mlm_probability=0.3\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e8feb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work1/lgarcia/wallacelw/ModernBERT/myenv/lib64/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import get_wsd_schedule, AdamW\n",
    "\n",
    "total_steps = 400_000\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'training/{model_name}',\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    max_steps=total_steps,\n",
    "\n",
    "    gradient_accumulation_steps = 1,\n",
    "\n",
    "    per_device_train_batch_size=32,\n",
    "    \n",
    "    logging_strategy=\"steps\",\n",
    "    logging_first_step=True, # output the initial loss\n",
    "    logging_steps=1_000,\n",
    "    logging_dir=f\"training-logs/{model_name}\",\n",
    "    report_to=[\"tensorboard\"],\n",
    "\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1_000,                      # Save checkpoints every 100 steps\n",
    "    save_total_limit=5,                  # Limit the total number of saved checkpoints\n",
    "\n",
    "    fp16=True,                            # Enable mixed precision for faster training\n",
    "\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=1e-5,\n",
    "    # adam_beta1=0.9,\n",
    "    # adam_beta2=0.999,\n",
    "    # adam_epsilon=1e-06,\n",
    ")\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(), \n",
    "    lr=training_args.learning_rate,\n",
    "    weight_decay=training_args.weight_decay,\n",
    ")\n",
    "\n",
    "lr_scheduler = get_wsd_schedule(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps= total_steps * 0.1,\n",
    "    num_stable_steps= total_steps * 0.6,\n",
    "    num_decay_steps= total_steps * 0.3,\n",
    "    min_lr_ratio= 0,\n",
    "    num_cycles= 0.5,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                        \n",
    "    args=training_args,                \n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    data_collator=data_collator,\n",
    "    optimizers=(optimizer, lr_scheduler),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "252f0c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7abad105",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ed44111",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['decoder.weight'].\n",
      "/work1/lgarcia/wallacelw/ModernBERT/myenv/lib64/python3.9/site-packages/transformers/trainer.py:3441: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n",
      "\tper_device_train_batch_size: 32 (from args) != 64 (from trainer_state.json)\n",
      "/work1/lgarcia/wallacelw/ModernBERT/myenv/lib64/python3.9/site-packages/transformers/trainer.py:3105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n",
      "/share/sw/ai/pytorch/2.5.1/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400001' max='400000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400000/400000 00:01, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=400001, training_loss=3.8058215601607604e-06, metrics={'train_runtime': 21.9659, 'train_samples_per_second': 4661768.783, 'train_steps_per_second': 18210.034, 'total_flos': 3.4903458558276796e+19, 'train_loss': 3.8058215601607604e-06, 'epoch': 1.0353439662892847})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73d9f41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"trained/Modern/5.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ffc3f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModernBertForMaskedLM(\n",
       "  (model): ModernBertModel(\n",
       "    (embeddings): ModernBertEmbeddings(\n",
       "      (tok_embeddings): Embedding(32768, 768, padding_idx=0)\n",
       "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): ModernBertEncoderLayer(\n",
       "        (attn_norm): Identity()\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1-2): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (4-5): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (6): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (7-8): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (9): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (10-11): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (12): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (13-14): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (15): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (16-17): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (18): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (19-20): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (21): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (head): ModernBertPredictionHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=False)\n",
       "    (act): GELUActivation()\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Linear(in_features=768, out_features=32768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ModernBertForMaskedLM\n",
    "\n",
    "model = ModernBertForMaskedLM.from_pretrained(\"trained/Modern/5.0\")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "45d93bd7",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "save_directory = \"trained/Modern/5.0\"\n",
    "\n",
    "model.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0efee79c",
   "metadata": {},
   "source": [
    "huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fb66e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/wallacelw/ModBERTBr2/commit/a9d973c35d2d760ddd2e23a11ef435a1202ffde7', commit_message='Upload ModernBertForMaskedLM', commit_description='', oid='a9d973c35d2d760ddd2e23a11ef435a1202ffde7', pr_url=None, repo_url=RepoUrl('https://huggingface.co/wallacelw/ModBERTBr2', endpoint='https://huggingface.co', repo_type='model', repo_id='wallacelw/ModBERTBr2'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"wallacelw/ModBERTBr2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52367e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/wallacelw/ModBERTBr2/commit/4dae1410a9b87e591c6146fb4fee0f86eaa014a3', commit_message='Upload tokenizer', commit_description='', oid='4dae1410a9b87e591c6146fb4fee0f86eaa014a3', pr_url=None, repo_url=RepoUrl('https://huggingface.co/wallacelw/ModBERTBr2', endpoint='https://huggingface.co', repo_type='model', repo_id='wallacelw/ModBERTBr2'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"wallacelw/ModBERTBr2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv) :D",
   "language": "python",
   "name": "myenv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
